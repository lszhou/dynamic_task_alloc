%--------------------CHAPTER 3-----------------------------
\chapter{Dynamic Task Allocation Object}

Our dynamic task allocation (DTA) type supports two operations, \texttt{DoTask()} and \texttt{InsertTask(}$\ell$\texttt{)}, where $\ell$ is the task identifier that is unique for each task.

Now we formalize the notion of type DTA by specifying the above two operations. We assume that there exists an atomic operation \texttt{PutTask(}$M,\ell$\texttt{)}, and a process associates task $\ell$ with memory location $M$ by calling \texttt{PutTask(}$M,\ell$\texttt{)}. It returns $success$ if task $l$ is associated with location $M$, and returns $failure$ if location $M$ was already associated with another task. We say task $\ell$ is inserted if it is associated with a memory location $M$.

Similarly, we assume there exists an atomic operation \texttt{TryTask(}$M$\texttt{)}, and task $\ell$ associated with memory location $M$ could be performed atomically by calling \texttt{TryTask(}$M$\texttt{)}. Out of several processes calling \texttt{TryTask(}$M$\texttt{)}, only one receives $success$ and the index $\ell$ of that task, while all the others receive $failure$.

A task is $done$ or $performed$ if its index has been returned by a process after calling \texttt{DoTask()}. A task is $available$ at location $M$ if it has been inserted to $M$ and $success$ is returned by a process after calling \texttt{InsertTask(}$\ell$\texttt{)}, but is not done yet. A task is $available$, if it is $available$ at some memory location.

The aim of operation \texttt{DoTask()} is to perform an available task on location $M$ by calling \texttt{TryTask(}$M$\texttt{)}. Every \texttt{DoTask()} may perform several \texttt{TryTask(}$M$\texttt{)} operations. However, only one of them will succeed. Once one \texttt{TryTask(}$M$\texttt{)} succeeds, then there is no available task on $M$ and the task index $\ell$ will be returned by \texttt{DoTask()}. Additionally, if there is no available task, then operation \texttt{DoTask()} returns $\perp$.

The goal of \texttt{InsertTask(}$\ell$\texttt{)} operation is to find a free memory location $M$ and insert task $\ell$ atomically by calling \texttt{PutTask(}$M,\ell$\texttt{)}. \texttt{PutTask(}$M,\ell$\texttt{)} fails if location $M$ has been associated with another task, so each \texttt{InsertTask(}$\ell$\texttt{)} operation may perform several \texttt{PutTask(}$M,\ell$\texttt{)} operations, but only one of them will succeed. Once one \texttt{TryTask(}$M$\texttt{)} succeeds, then task $\ell$ is available on location $M$ and $success$ notification is returned by \texttt{InsertTask(}$\ell$\texttt{)} operation.

Type DTA is required to satisfy: (Validity) If a \texttt{DoTask()} operation returns $\ell$, then before the \texttt{DoTask()} operation, an \texttt{InsertTask(}$\ell$\texttt{)} was executed and returned $success$. (Uniqueness) Each task is performed at most once, i.e, for each task $\ell$, at most one \texttt{DoTask()} operation returns $\ell$.

In addition, the property that every inserted task is eventually done is also a desired progress property of the implementation of type DTA.

\section*{Implementation of DTA}
(under work...)
%The main data structure we used for the implementation of DTA is a binary tree. Each tree node $v$ contains a CAS object. The value of each CAS object is a pair $(x, y)$ where $x$ is called the \emph{insert count} and $y$ is called \emph{remove count}. The \emph{insert count} tracks the number of tasks inserted in the subtree while \emph{remove count} tracks the number of tasks that are already performed in the subtree. In another word, $x$ is the total number of successful \texttt{InsertTask} operations performed in the subtree rooted at $v$ and $y$ is the total number of successful \texttt{DoTask} operations performed in the subtree rooted at $v$.
%
%Thus, if $v$.\texttt{read}() returns pair $(x, y)$, then there are $(x-y)$ tasks available in the subtree rooted at $v$ because there have been $x$ tasks inserted and $y$ tasks performed. We define this difference $(x-y)$ the \emph{surplus} at node $v$ and denote it as $u_v$. Symmetrically, we define the complement $(2^h-u_v)$ the $\emph{space}$ at node $v$ where $h$ is the height of node $v$.
%
%For each leaf of the binary tree, it contains not only a CAS object but also an unbounded array $task[ ]$ whose cells are memory locations which could be associated with tasks. When a task $\ell$ is associated with the a cell $task[i]$, then we say task $\ell$ is inserted into cell $task[i]$ or array $task[ ]$.
%
%\Tree[.$v_1$ [.$v_{1,1}$
%               ]
%          [.$v_2$ [.$v_{2,1}$ ]
%
%                [.VP [.V\1 [.V \textit{is} ]
%                           [.AP [.Deg \textit{really} ]
%                                [.A\1 [.A \textit{simple} ]
%                                      \qroof{\textit{to use}}.CP ]]]]]]


\begin{algorithm}
\caption{\texttt{DoTask()}}
\While {$true$}{
   $v \leftarrow root$;\\

   \If{$v.surplus() \leq 0$}{
   \Return $\perp$;\
   }
   \BlankLine
   /* Descent */\;
    \While{$v$ is not a leaf}{
        $(x_L, y_L) \leftarrow v.left.read()$\;
        $(x_R, y_R) \leftarrow v.right.read()$\;
        $s_L \leftarrow min(x_L-y_L, 2^{height(v)})$\;
        $s_R \leftarrow min(x_R-y_R, 2^{height(v)})$\;
        $r \leftarrow random(0,1)$\;

        \uIf{$(s_L+s_R)=0$}{
            Mark-up($v$)\;
        }
        \uElseIf{$r<s_L/(s_L+s_R)$}{
            $v \leftarrow v.left$\;
        }
        \Else{
            $v \leftarrow v.rght$\;
        }
    }

    \BlankLine
    /* $v$ is a leaf */\;
    $(x,y) \leftarrow v.read()$\;
    $(flag, l)$ $\leftarrow$ $v$.\texttt{TryTask}(task$[y+1]$)\;

    \BlankLine
    /* Update Insertion Count */\;
    $v$.CAS$((x, y), (x, y+1))$\;
    $v \leftarrow v.parent$\;
    Mark-up($v$)\;
    \If{$flag$ = $success$}{
        \Return $\ell$
    }
}
\end{algorithm}

%\begin{algorithm}
%\caption{DoTask()}               %标题
%\label{alg1}                         %标记算法，方便在其它地方引用
%\begin{algorithmic}[1]
%\WHILE {(true)}
%   \STATE $v \leftarrow root$\\
%   \IF {($v.surplus() \leq 0$)}
%   \RETURN $\perp$
%   \ENDIF \\
%   \vspace{3mm}
%   /* Descent */
%   \WHILE {$v$ is not a leaf}
%       \STATE $(x_L, y_L) \leftarrow v.left.read()$\\
%       \STATE $(x_R, y_R) \leftarrow v.right.read()$\\
%       \STATE $s_L \leftarrow min(x_L-y_L, 2^{height(v)})$ \\
%       \STATE $s_R \leftarrow min(x_R-y_R, 2^{height(v)})$ \\
%       \STATE $r \leftarrow random(0,1)$
%       \IF {$((s_L+s_R)=0)$}
%           \STATE Mark-up($v$)
%       \ELSIF {($r<s_L/(s_L+s_R)$)}
%           \STATE $v \leftarrow v.left$
%       \ELSE
%           \STATE $v \leftarrow v.rght$
%       \ENDIF
%   \ENDWHILE
%   \vspace{3mm}\\
%   /* $v$ is a leaf */
%   \STATE $(x,y) \leftarrow v.read()$
%   \STATE $(flag, l)$ $\leftarrow$ $v$.TryTask(task$[y+1]$)
%   \STATE $v$.CAS$((x, y), (x, y+1))$    // Update Insertion Count
%   \STATE $v \leftarrow v.parent$
%   \STATE Mark-up($v$)
%   \IF {$flag$ = success}
%       \RETURN $\ell$
%   \ENDIF
%\ENDWHILE
%\end{algorithmic}
%\end{algorithm}



\begin{algorithm}
\caption{\texttt{InsertTask(}$\ell$\texttt{)}}
\While{$true$}{
    $v \leftarrow root$\;
    \BlankLine
    /* Descent */\;
    \While{$v$ is not a leaf}{
        $(x_L, y_L) \leftarrow v.left.read()$\;
        $(x_R, y_R) \leftarrow v.right.read()$\;
        $s_L \leftarrow 2^{height(v)} - min(x_L-y_L, 2^{height(v)})$\;
        $s_R \leftarrow 2^{height(v)} - min(x_R-y_R, 2^{height(v)})$\;
        $r \leftarrow random(0,1)$\;

        \uIf{$(s_L+s_R)=0$}{
            Mark-up($v$)\;
        }
        \uElseIf{$r<s_L/(s_L+s_R)$}{
            $v \leftarrow v.left$\;
        }
        \Else{
            $v \leftarrow v.rght$\;
        }
    }

    \BlankLine
    /* $v$ is a leaf */\;
    $(x,y) \leftarrow v.read()$\;
    $flag$ $\leftarrow$ $v$.\texttt{PutTask}(task$[x+1]$)\;

    \BlankLine
    /* Update Insertion Count */\;
    $v$.CAS$((x, y), (x+1, y))$\;
    $v \leftarrow v.parent$\;
    Mark-up($v$)\;
    \If{$flag$ = $success$}{
        \Return $success$
    }
}
\end{algorithm}



%\begin{algorithm}
%\caption{Insert(task $\ell$)}               %标题
%\label{alg2}                         %标记算法，方便在其它地方引用
%\begin{algorithmic}[1]
%\WHILE {(true)}
%   \STATE $v \leftarrow root$\\
%   \vspace{3mm}
%   /* Descent */
%   \WHILE {($v$ is not a leaf)}
%       \STATE $(x_L, y_L) \leftarrow v.left.read()$\\
%       \STATE $(x_R, y_R) \leftarrow v.right.read()$\\
%       \STATE $s_L \leftarrow min(x_L-y_L, 2^{height(v)})$ \\
%       \STATE $s_R \leftarrow min(x_R-y_R, 2^{height(v)})$ \\
%       \STATE $r \leftarrow random(0,1)$
%       \IF {($(s_L+s_R) = 0$)}
%           \STATE Mark-up($v$)
%       \ELSIF {($r < s_L/(s_L + s_R)$)}
%           \STATE $v \leftarrow v.left$
%       \ELSE
%           \STATE $v \leftarrow v.rght$
%       \ENDIF
%   \ENDWHILE
%   \vspace{3mm}\\
%   /* $v$ is a leaf */
%   \STATE $(x,y) \leftarrow v.read()$
%   \STATE $flag$ $\leftarrow$ $v$.PutTask(task$[x+1]$, $\ell$)
%   \STATE $v$.CAS$((x,y), (x+1,y))$ // Update Insertion Count
%   \STATE $v \leftarrow v.parent$
%   \STATE Mark-up($v$)
%   \IF {($flag$ = success)}
%       \RETURN $success$
%   \ENDIF
%\ENDWHILE
%\end{algorithmic}
%\end{algorithm}



\begin{algorithm}
\caption{\texttt{Mark-up}($v$)}
\If{$v$ is not $null$}{
    \For {($i=0; i<2; i++$)}{
        $(x, y) \leftarrow v.read()$\;
        $(x_L, y_L) \leftarrow v.left.read()$\;
        $(x_R, y_R) \leftarrow v.right.read()$\;
        $v$.CAS$((x,y), (max(x, x_L + x_R), max(y, y_L + y_R))$\;
    }
}
\end{algorithm}



%
%\begin{algorithm}
%\caption{Mark-up($v$)}               %标题
%\label{alg3}                         %标记算法，方便在其它地方引用
%\begin{algorithmic}[1]
%\IF{($v$ is not Null)}
%    \FOR {($i=0; i<2; i++$)}
%    \STATE $(x, y) \leftarrow v.read()$
%    \STATE $(x_L, y_L) \leftarrow v.left.read()$
%    \STATE $(x_R, y_R) \leftarrow v.right.read()$
%    \STATE $v$.CAS$((x,y), (max(x, x_L + x_R), max(y, y_L + y_R))$\\
%    \ENDFOR
%\ENDIF
%\end{algorithmic}
%\end{algorithm}

\chapter{Correctness Proof}
\section{Correctness}
The standard correctness condition for shared memory algorithms is linearizability, which was introduced by Herlihy and Wing in 1990 \cite{Herlihy:1990:LCC:78969.78972}. The intuition of linearizability is that real-time behavior of method calls must be preserved, i.e, if one method call precedes another, then the earlier call must have taken effect before the later one. By contrast, if two method calls overlap, we are free to order them in any convenient way since the order is ambiguous. Informally, a concurrent object is linearizable if each method call appears to take effect instantaneously at some moment between its invocation and response.

\subsection{Analysis and Proofs}

By the definitions in Subsection 3.1.1, one way to show an object $obj$ is linearizable is to prove every history $H$ of $obj$ is linearizable. Thus, we need to identify for each \texttt{DoTask} and \texttt{InsertTask} operation $op$ (i.e, interval $I_H(op)$) in $H$ a linearization point $t_H(op)$, and prove that the sequential history $S$ obtained by sorting these operations according to their $t_H(op)$ satisfies the sequential specification  $S_{obj}$  of $obj$.

We notice that each complete \texttt{DoTask} or \texttt{InsertTask} operation can be associated with a unique task array slot based on the task it removed or inserted. Additionally, the removal and insertion count are both monotonically increasing. Thus, we could associate the node counts with operations which have been propagated to that node.

Now we define ``an operation is counted at a node" recursively to formalize the operation propagation.

A \texttt{DoTask} operation is counted at leaf $v$ when the removal count of $v$ is updated with the index of the task array slot where the performed task is located. Symmetrically, an \texttt{InsertTask} operation is counted at $v$ when the insertion count of $v$ is updated with the index of the task array slot where the inserted task is located.

Now we only define \texttt{DoTask} operation is counted at an inner node $v$ because counting an \texttt{InsertTask} operation is symmetric as well.

Recall that the removal count of $v$ is updated though CAS operation (line 6, method 3). Actually there could be more than one operations updating the count with the same value. We linearize all such CAS operations, which update the removal count of $v$ with the same value $y$. We say for all these operations, only the first one in the linearization order counts the corresponding \texttt{DoTask} operation. In another word, a \texttt{DoTask} operation is counted at an inner node $v$ as soon as the CAS updating operation that counts the \texttt{DoTask} is linearized. Based this definition, no operation will be counted twice at a node.

Please note that, the CAS operation counting the \texttt{DoTask} at node $v$ is not necessary performed by the \texttt{DoTask} operation itself, i.e, suppose process $p$ executes a \texttt{DoTask} operation and has successfully performed task $\ell$ at certain leaf. Then the CAS operation counting this $p.DoTask()$ at node $v$ could be a different process $q$ as long as $q$ updates the removal count first in the linearization order.

Given the above concepts and properties, we could prove the following result:
(under work...almost done)
%\begin{lem}
%Let $v$ be a tree node, \\
%(1) If $(x, y)$ is the return value of $v.read( )$, then there exists a set of $x$ \texttt{InsertTask} operations and a set of $y$ \texttt{DoTask} operation that have been counted at node $v$ by the end of the execution of $v.read( )$.\\
%\noindent (2) If there are $x$ \texttt{DoTask} operations that have been counted at $v$ before the execution of $v.read( )$, then the removal count value returned by $v.read( )$ is not less than $x$.
%\end{lem}
%\begin{proof}
%TBD
%%[Sketch] This is ensured by the repetition of double compare and swap operation during the execution of mark-up($v$).
%\end{proof}
%
%
%
%\begin{lem}
%Consider a history $H$ and an arbitrary operation $op$ in $H$, let $t_H(op)$ be the point when $op$ is counted at the root, then $t_H(op)$ is between $inv_H(op)$ and $rsp_H(op)$.
%\end{lem}
%\begin{proof}
%Without loss of generality, suppose process $p$ executes DoTask operation $op$ which has performed task $\ell$ successfully at the leaf. When it reaches the $root$ and executes Mark-up($root$), there will be two cases:
%
%Case 1:  It increments the removal count of $root$ successfully via CAS (line 6, method 3) at point $\tau$. If it is the first one in the linearization order of all CAS operation updating of the removal count with the same value, then $t_H(op) = \tau$, therefore $inv_H(op) < t_H(op) < rsp_H(op)$. Otherwise, we could let $t_H(op) = \tau'$, where $\tau'$ is the time when the first CAS operation in the linearization order updated the removal count. Thus $t_H(op) < \tau < rsp_H(op)$, Because only if the task has been performed then the removal count of root could be updated. so $t_H(op) > inv_H(op)$. Therefore, in this case, $inv_H(op) < t_H(op) < rsp_H(op)$ holds.
%
%Case 2: It fails to increment the removal count. The CAS operation of $p$ fails if and only if the value of the counts were updated by another process at a point before $\tau$, suppose it is $\tau'$. Please note that, we say the counts were updated, it means the removal count or the insertion count was updated because the two counts are stored in one memory location. Thus, there are two subcases.
%
%Subcase 2.1: If it is the removal count that was incremented at $\tau'$, it means the DoTask operation has already been counted by another process. Thus, $t_H(op) \leq \tau' < rsp_H(op)$.
%
%Subcase 2.2: If it is not the removal count but the insertion count that was updated at $\tau'$. We notice that the CAS operation (line 6, method 3) will be repeated by $p$, during the second iteration, if the CAS of $p$ succeed at $\tau''$, then we could deduce that $t_H(op) \leq \tau''$, therefore  $t_H(op) < rsp_H(op)$. If it fails again at point $\tau''$ , then there must be another process updated the counts of root again. This time, the counts of the children will be noticed and the DoTask operation will propagate to the root. We could deduce $t_H(op) < \tau''$. Thus,  $t_H(op) < rsp_H(op)$ as well.
%\end{proof}
%
%Under the above definitions and properties, we claim the point $t_H(op)$ when $op$ is counted at the root is the linearization point of operation $op$.
%
%\begin{lem}
%The dynamic task allocation object in the above figure is linearizable.
%\end{lem}
%\begin{proof}
%Consider an arbitrary history $H$ containing DoTask and InsertTask operations. We should prove for any execution of our algorithm, the total order given by the linearization point is verifies the uniqueness and validity. If $H$ is not complete, then we let all processes that have not finished their operations continue to take steps in an arbitrary order until all operations are completed. Every operation will finally be done is ensured by our computation model and the randomness of our algorithm. This way we obtain a completion $H'$ of $H$ and it suffices to prove $H'$ is linearizable. Thus, to prove this lemma, we should prove the total order obtained by sorting the operations by their $t_H(op)$ values is valid.
%
%\texttt{The uniqueness is obvious. When multiple processes are calling TryTask($\ell$) at the memory location, only one of them will receive success and the index $\ell$ of the task, all the other competitor processes will get failure. Task $\ell$ is performed successfully as long as the value of corresponding memory location is turned to 1. The following process will never repeatedly turn it be to 0 and turn 0 to 1 which is guaranteed by the our semantics of task insertion and removing.}
%
%Now we prove the validity, i.e. each task that is performed successfully must have been inserted before. We should prove the insertion operation of a task is always counted at the root before the removal operation. To prove this result holds for the root, we now prove it by induction from the leaf.
%
%At the leaf, this holds because if and only if the insertion count of newly inserted task $\ell$ has been incremented (line 19, method 2) then the following removal operation could read that (line 20, method 1) to know the available task $\ell$ at the leaf and then try to perform it. In another word, suppose the task $\ell$ is inserted but the insertion count is not incremented (i.e. insertion has not been counted yet), then the following removal operation has no way to know the available task $\ell$, perform it and increment the removal count. Thus, the removal will not be counted.
%
%For an arbitrary inner node $v$, we suppose, by the induction step and lemma 1, the result holds for children $v.left$ and $v.right$. We could notice that any process updates the insertion count and removal count as an atomic operation (line 6, method 3). If some remove operation has been counted at node $v$, then the corresponding insert operation for that task must have been counted at $v$ simultaneously by the double compare-and-swap operation. Apply this to the root, then validity condition holds.
%\end{proof}
%
