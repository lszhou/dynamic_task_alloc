
%--------------------CHAPTER 2-----------------------------
\chapter{Model of Computation and Definitions}
\section{Asynchronous Shared Memory Model}
In this chapter, we will describe our model of computation and give the definitions, which are based on Herlihy
and Wing's \cite{Herlihy:1990:LCC:78969.78972} and Golab, Higham and Woelfel's \cite{golab2011linearizable}.

The computational model we consider is the standard asynchronous shared memory model with a set $\mathcal{P}$
of $n$ processes, denoted as $\mathcal{P} = [p] =\{p_0, p_2,...,p_{n-1}\}$ , where up to $n-1$ processes may fail by crashing.
A process may crash at any moment during the computation and once crashed it does not restart,
and does not perform any further actions.

\textbf{Type and Object}.
A \emph{type} $\tau$ is defined as an automaton as follows \cite{InProc-GHHW2007a},
$$\tau = (\mathcal{S}, s_{init},\mathcal{O},\mathcal{R} ,\delta )$$

where $\mathcal{S}$ is a set of states, $s_{init} \in \mathcal{S}$ is the initial state, $\mathcal{O}$ is a set of
operations, $\mathcal{R}$ is the set of responses, and
$\delta :\mathcal{S} \times \mathcal{O} \to \mathcal{S} \times \mathcal{R}$ is a state transition mapping.

An \emph{object} is an implementation of a type. For each type $\tau$, the transition mapping $\delta$ captures the
behaviour of objects of type $\tau$, in the absence of concurreny,
as follows: if a process applies an operation of type $opt$ to an object of type $\tau$ which is in state $s$, the object
may return to the process a response $rsp$ and change its states to $s'$ if and only if $(s', rsp) \in \delta(s, opt)$.

\textbf{History}.
A \emph{history} $H$, obtained by processes executing
operations on objects, is a sequence of invocation
and response events.

An invocation event is a 5-tuple,
\begin{center}
INV = $(invocation, p, obj, opt, t)$
\end{center}
where $invocation$ is the event type, $p$ is the process executing the operation, $obj$ is the object on which the operation
is executed, $opt$ is the operation and $t$ is the \emph{time} when INV happens which is defined
as the position of event INV in history $H$.

A response event is also a 5-tuple,
\begin{center}
RSP = $(response, p, obj, rsp, t)$
\end{center}
where $response$ is the event type, $p$ is the process receiving response $rsp$ from an oeration on object $obj$ and $t$
is the time when RSP happens which is defined as the position of event RSP in history $H$.

For an event with respect to
process $p$, we also say this event is \emph{applied} by process $p$.

In the following discussion, we suppose in a history $H$, the situation that an invocation event
$(invocation, p_i, obj_p, opt_0, t_0)$ is followed immediately by another invocation event\\
$(invocation, p_j, obj_q, opt_1, t_1)$ where $i = j$ and $p = q$ will not happen.

Response event $(response, p_j, obj_q, rsp, t_1)$ \emph{matches} invocation event $(invocation, p_i, obj_p, opt, t_0)$
in history $H$, if the two events are applied by the same process to the same object, i.e, $i = j$ and $p = q$.
In this case, the response event is also called the $matching$ response of the invocation event.

An \emph{operation execution} in $H$ is a pair $oe$ = (INV, RSP) consisting of an invocation event INV
and its matching response event RSP, or just an invocation event INV with no matching response event,
denoted as $oe$ = (INV, $null$).
In the latter case, we say the operation execution is \emph{pending}. In the former case, we say the operation
execution is \emph{complete}. A history $H$ is \emph{complete} if all operation executions in $H$ are \emph{complete},
otherwise, it is \emph{incomplete}. If events INV and RSP are applied by process $p$, then we say
operation execution $oe$ = (INV, RSP) is \emph{performed} by process $p$. Thus, two operation
executions performed by the same process on the same project will not interleave in a history $H$.

We say that an operation $opt$ is atomic in history $H$, if opâ€™s invocation event is either the last event in
$H$, or else is followed immediately in $H$ by a matching response event.

History $H'$ is an extension of history $H$ if $H$ is a prefix of $H'$.
History $H'$ is a \emph{completion} of history $H$ if $H'$ contains all the
events in $H$ and $H'$ is an extension of $H$, and each operation execution in $H'$ is complete.

$H|obj$ of history $H$ is the subsequence of all
invocation and response events in $H$ on object $obj$. If all invocation and response
events in a history $H$ have the same object name $obj$, then the $H|obj = H$.

Let $H$ be a complete history. We associate a time interval $I_{oe} = [t_0, t_1]$ with each
operation execution $oe$ = (INV, RSP) in $H$, where $t_0$ and $t_1$ are the points in time when INV and RSP happen.
Similarly, for an incomplete history, we denote the time interval $I_{oe}$ with respect to a pending
operation execution $oe$ = (INV, $null$) by $I_{oe} = [t_0, \infty]$.

Operation execution $oe_0$ \emph{precedes} operation execution $oe_1$ in $H$ if the response event of
$oe_0$ happens before the invocation event of $oe_1$ in $H$.
We say that $oe_0$ and $oe_1$ are \emph{concurrent} in $H$ if neither precedes the other.

A history is \emph{sequential} if its first event is an invocation event, and each invocation event, except
possibly the last one, is immediately followed by a matching response event.

A \emph{sequential specification} of an object is the set of all possible sequential histories
for that object.

A sequential history $S$ is \emph{valid}, if for each object $obj$, $S|obj$ is
in the sequential specification of $obj$.

\textbf{Linearization}.
A history $H$ \emph{linearizes} to a sequential history $S$, if and only if $S$ satisfies the
following conditions: (1) $S$ and any completion of $H$ have the same operation executions, (2) sequential history $S$ is
valid, and (3) there is a mapping from each time interval $I_{oe}$ to a time point $t_{oe} \in I_{oe}$, such
that the sequential history $S$ is obtained by sorting the operations in $H$ based on their $t_{oe}$ values.

A history is \emph{linearizable} if and only if $H$ linearizes to some sequential history $S$. In this case,
$S$ is called the \emph{linearization} of $H$. For each operation $opt$ in history $H$, we call time point $t_{oe}$, which is
defined as above, the \emph{linearization point} of $opt$. An object $obj$ is linearizable if every
history $H$ on $obj$ is linearizable.

\section{Primitive Objects}
In this section, we describe the two primitive objects which will be used in our following discussion,
namely \emph{read-write register} and \emph{compare-and-swap} (CAS) objects. Most implementations
of more sophisticated objects use these objects as the base objects in their implementations,
since most modern architectures support either read-write registers and CAS objects \cite{itanium} \cite{weaver1994sparc}.

\textbf{Read-Write Register}.
An object that supports only \texttt{read()} and \texttt{write(x)} operations is called
a read-write register (or just \emph{register}). Operation \texttt{read()} returns the current state of register and
leaves the state unchanged. Operation \texttt{write(x)} changes the state of the register to $x$ and returns nothing.
If the set of states that can be stored in the register is unbounded then we say the register is \emph{unbounded register};
otherwise the register is \emph{bounded register}.

\textbf{CAS Object}. An object that supports \texttt{read()} and \texttt{CAS(x,y)} operations is called compare-and-swap (CAS) object.
Operation \texttt{read()} is the same as defined above. Operation \texttt{CAS(x,y)} changes the state of
the object if and only if the current state is equal to $x$ and then operation \texttt{CAS(x,y)} succeeds, and the state is changed
to $y$ and $true$ is returned. Otherwise, operation \texttt{CAS(x,y)} fails, the current state remains unchanged and
$false$ is returned.

\section{Adversary Models for Randomized Algorithms}

\textbf{Randomness}.
A randomized algorithm is an algorithm where processes are allowed to make random decisions for the next step.
This is modelled by giving each process a special operation called \emph{coin-flip operation}. We say a process can flip a coin
when it calls this operation. When a process flips a coin, it receives a random value $c$ from some arbitrary set $\Omega$
which is calle the \emph{coin-flip domain}. The process can then use this random value $c$ in its program for future decisions.

A vector $\overrightarrow{\rm c} = (c_0, c_1, c_2,...) \in \Omega^{\infty}$ is called a \emph{coin-flip vector}.
A history $H$ is said to \emph{observe} the coin-flip vector $\overrightarrow{\rm c}$ if
for an arbitrary integer $i \in [0, \infty)$, the $i$-th coin-flip operation in $H$ returns value $c_i$.

For a history $H$ that contains $k$ coin-flip operations, we use $H[k]$ to denote the prefix of $H$
that ends with the $k$-th invocation of a coin-flip operation. If fewer than $k$ coin-flip operation
executions occur during H, then $H[k]$ denotes $H$.

In the following discussion, we use operation \texttt{random(s)}, which is assumed
to be atomic, to return a value which is distributed uniformly at random over domain
$\{0, 1, 2,..., s-1\}$.

\textbf{Schedule}.
In the standard shared memory model, each process executes its program by applying
shared memory operations (\texttt{read()}, \texttt{write(x)}, \texttt{CAS(x,y)}, etc) on objects,
as determined by their program. Operation executions of concurrent processes
can be interleaved arbitrarily.

A \emph{schedule} with \emph{length} $k$ is represented by a sequence of process ids
$$p = (p_0, p_1, p_2,..., p_{k-1})$$ where $k \in [0, \infty)$ and for each $i \in [0, k-1]$,
$p_i \in \mathcal{P}$.

Let a schedule $p = (p_0, p_1, p_2,..., p_{k-1})$.
A history $H$ is said to \emph{observe} schedule $p$ if the number of events in $H$ is $k$,
and for each integer $i \in [0, k-1]$, the $i$-th event is applied by process $p_i$.

\textbf{Adversary}.
In randomized algorithm, the random choices processes make can influence the schedule.
To model the worst-case possible way that the system can be influenced by the random choices,
schedules are assumed to be generated by an adversarial scheduler, called the \emph{adversary}.

Mathematically, an adversary is defined as a mapping:
\begin{center}
$\mathcal{A} :  \Omega^{\infty} \to \mathcal{P}^{\infty}$
\end{center}

Given an algorithm $\mathcal{M}$, an adversary $\mathcal{A}$, and
a coin-flip vector $\overrightarrow{\rm c} \in \Omega^{\infty}$,
a unique history $H_{\mathcal{M},\mathcal{A}, \overrightarrow{\rm c}}$ is generated, such that all
processes apply events as dictated by algorithm $\mathcal{M}$, and history
$H_{\mathcal{M}, \mathcal{A}, \overrightarrow{\rm c}}$
observes the schedule $\mathcal{A(\overrightarrow{\rm c})}$ and the coin flip vector $\overrightarrow{\rm c}$.

There are several adversary models with different strengths \cite{DBLP:journals/corr/cs-DS-0209014}.
In our thesis, we only conside the \emph{adaptive adversary}.

Informally, the adaptive adversary makes scheduling
decisions as follows: At any point, it can see the entire history up to that point.
This includes all coin-flip operations and their return values up to that point. Depending on this,
the adversary decides which process takes the next step.

Adversary $\mathcal{A}$ is \emph{adaptive for algorithm $\mathcal{M}$} \cite{golab2011linearizable}
if, for any two coin-flip
vectors $\overrightarrow{\rm c} \in \Omega^{\infty}$ and $\overrightarrow{\rm d} \in \Omega^{\infty}$ that have a common prefix
of length $k$ (i.e, the first $k$ elements of them are the same), then we have
$$H_{\mathcal{M},\mathcal{A}, \overrightarrow{\rm c}}[k+1] = H_{\mathcal{M},\mathcal{A}, \overrightarrow{\rm d}}[k+1]$$
In this case, we also say Adversary $\mathcal{A}$ is an \emph{adaptive adversary}.

From the above definition, we can see an adaptive adversary cannot use
future coin flips to make current scheduling decisions.

\section{The Dynamic Task Alloction Problem}
* \emph{define what is task? what is perform a task and insert a task?}
\subsection{Task}
Performing a task can be be any computation which can be performed by a single
process in constant time \cite{georgiou2007all} and the tasks are assumed to satisfy the following three
properties:
\emph{Similarity}: The computations consume equal or comparable resources.
\emph{Independence}: One computation is independent of any of the others.
\emph{Idempotence}: Tasks admit at-least-once execution semantics.

For simplicity, in our thesis, \emph{tasks} are abstracted and defined as shared registers in an array
denoted as \texttt{tasks}$[]$. Eeach task is initially set to $0$ and must be flipped to $1$.
Each task has a unique identifier from the set
$\mathcal{L} = [m] = \{\ell_0, \ell_1, \ell_2,..., \ell_m\}$, where $m \in [0, \infty)$.

We say a task is \emph{performed} when the state of that task is flipped to $1$ and a task is inserted
when the state is flipped back to $0$.

If the above value $m$ is fixed and the $m$ tasks are available initially, and the computation ends when all
tasks are performed, then we get the static version of task allocation problem, namely the write-all problem which
is defined as follws: Given a zero-valued array of $m$ elements and $n$ processes,
write value $1$ into each array cell in the presence of adversity \cite{kanellakis1992efficient}.

In this thesis, we consider the dynamic version of task allocation problem, i.e, we are still given $n$
asynchronous processes that cooperate to perform tasks, while new tasks are inserted
dynamically by the adversary during the computation.

Now we specify the dynamic task alloction problem in terms of a type DTA, and the properties
that an implementation of type DTA must satisfy.

\subsection{The Type DTA}

The type DTA supports two types of operations., \texttt{DoTask()} and
\texttt{InsertTask(}$\ell$\texttt{)}, where $\ell$ is the identifier that is unique for each task.
A process \emph{attempts} to perform a task by executing \texttt{DoTask()}, and the process associates
a task $\ell$ with a memory location by executing \texttt{InsertTask(}$\ell$\texttt{)}.

We assume that there exists an atomic operation \texttt{PutTask(}$M,\ell$\texttt{)},
and a process associates (\emph{what is associate?}) task $\ell$ with memory location (\emph{what is memory location?}) $M$
by calling \texttt{PutTask(}$M,\ell$\texttt{)}.
It returns $success$ if task $\ell$ is associated with location $M$, and returns $failure$ if location $M$ was
already associated with another task.

Similarly, we assume there exists an atomic operation \texttt{TryTask(}$M$\texttt{)}, and
task $\ell$ associated with memory location $M$ could be performed atomically by calling \texttt{TryTask(}$M$\texttt{)}.
Out of several processes calling \texttt{TryTask(}$M$\texttt{)}, one receives $success$ and the index $\ell$ of
that task, while all the others receive $failure$. (\emph{state transition for all initial states, what happens
next if there is no...})

A task is $done$ or $performed$ if its index has been returned by a process after calling \texttt{DoTask()}.
A task is $available$ at location $M$ if it has been inserted to $M$ and $success$ is returned by a process
after calling \texttt{InsertTask(}$\ell$\texttt{)}, but is not done yet. A task is $available$, if it is $available$
at some memory location.

The aim of operation \texttt{DoTask()} is to perform an available task on location $M$ by calling
\texttt{TryTask(}$M$\texttt{)}. Every \texttt{DoTask()} may perform several \texttt{TryTask(}$M$\texttt{)}
operations. However, only one of them will succeed. Once one \texttt{TryTask(}$M$\texttt{)} succeeds, then
there is no available task on $M$ and the task index $\ell$ will be returned by \texttt{DoTask()}. Additionally,
if there is no available task, then operation \texttt{DoTask()} returns $\perp$.

The goal of \texttt{InsertTask(}$\ell$\texttt{)} operation is to find a free memory location $M$ and insert
task $\ell$ atomically by calling \texttt{PutTask(}$M,\ell$\texttt{)}. \texttt{PutTask(}$M,\ell$\texttt{)}
fails if location $M$ has been associated with another task, so each \texttt{InsertTask(}$\ell$\texttt{)}
operation may perform several \texttt{PutTask(}$M,\ell$\texttt{)} operations, but only one of them will succeed.
Once one \texttt{TryTask(}$M$\texttt{)} succeeds, then task $\ell$ is available on location $M$ and $success$
notification is returned by \texttt{InsertTask(}$\ell$\texttt{)} operation.

Type DTA is required to satisfy: (\emph{Validity}) If a \texttt{DoTask()} operation returns $\ell$, then before the
\texttt{DoTask()} operation, an \texttt{InsertTask(}$\ell$\texttt{)} was executed and returned $success$.
(\emph{Uniqueness}) Each task is performed at most once, i.e, for each task $\ell$, at most one \texttt{DoTask()}
operation returns $\ell$.

In addition, the property that every inserted task is eventually done is also a desired progress property
of the implementation of type DTA.
