
%--------------------CHAPTER 2-----------------------------
\chapter{Model of Computation and Definitions}
\section{Asynchronous Shared Memory Model}
In this chapter, we will describe our model of computation and give the definitions, which are based on Herlihy
and Wing's\cite{Herlihy:1990:LCC:78969.78972} and Golab, Higham and Woelfel's\cite{golab2011linearizable}.

The computational model we consider is the standard asynchronous shared memory model with a set $\mathcal{P}$
of $n$ processes, denoted as $\mathcal{P} = [p] =\{p_0, p_2,...,p_{n-1}\}$ , where up to $n-1$ processes may fail by crashing.
A process may crash at any moment during the computation and once crashed it does not restart,
and does not perform any further actions.

\textbf{Type and Object}.
A \emph{type} $\tau$ is defined as an automaton as follows\cite{InProc-GHHW2007a},
$$\tau = (\mathcal{S}, s_{init},\mathcal{O},\mathcal{R} ,\delta )$$

where $\mathcal{S}$ is a set of states, $s_{init} \in \mathcal{S}$ is the initial state, $\mathcal{O}$ is a set of
operations, $\mathcal{R}$ is the set of responses, and
$\delta :\mathcal{S} \times \mathcal{O} \to \mathcal{S} \times \mathcal{R}$ is a state transition mapping.

An \emph{object} is an implementation of a type. For each type $\tau$, the transition mapping $\delta$ captures the
behaviour of objects of type $\tau$, in the absence of concurreny,
as follows: if a process applies an operation $opt$ to an object of type $\tau$ which is in state $s$, the object
may return to the process a response $rsp$ and change its states to $s'$ if and only if $(s', rsp) \in \delta(s, opt)$.

\textbf{History}.
A \emph{history} $H$, obtained by processes executing
operations on objects, is a sequence of invocation
and response events.

An invocation event is a 5-tuple,
\begin{center}
INV = $(invocation, p, obj, opt, t)$
\end{center}
where $invocation$ is the event type, $p$ is the process executing the operation, $obj$ is the object on which the operation
is executed, $opt$ is the operation and $t$ is the \emph{time} when INV happens which is defined
as the position of event INV in history $H$. We also say the event INV is the invocation event of operation $opt$.

A response event is also a 5-tuple,
\begin{center}
RSP = $(response, p, obj, rsp, t)$
\end{center}
where $response$ is the event type, $p$ is the process receiving response $rsp$ from an oeration on object $obj$ and $t$
is the time when RSP happens which is defined as the position of event RSP in history $H$.

In the following discussion, we suppose in a history $H$, the situation that an invocation event
$(invocation, p_i, obj_p, opt_0, t_0)$ is followed immediately by another invocation event\\
$(invocation, p_j, obj_q, opt_1, t_1)$ where $i = j$ and $p = q$ will not happen.

Response event $(response, p_j, obj_q, rsp, t_1)$ \emph{matches} invocation event $(invocation, p_i, obj_p, opt, t_0)$
in history $H$, if the two events are applied by the same process to the same object, i.e, $i = j$ and $p = q$.
In this case, the response event is also called the $matching$ response of the invocation event.

An \emph{operation execution} in $H$ is a pair $oe$ = (INV, RSP) consisting of an invocation event INV
and its matching response event RSP, or just an invocation event INV with no matching response event,
denoted as $oe$ = (INV, $null$).
In the latter case, we say the operation execution is \emph{pending}. In the former case, we say the operation
execution is \emph{complete}. A history $H$ is \emph{complete} if all operation executions in $H$ are \emph{complete},
otherwise, it is \emph{incomplete}. If events INV and RSP are applied by process $p$, then we say
operation execution $oe$ = (INV, RSP) is \emph{performed} by process $p$. Thus, two operation
executions performed by the same process on the same project will not interleave in a history $H$.

We say that an operation $opt$ is \emph{atomic} in history $H$, if $opt$'s invocation event is either the last event in
$H$, or else is followed immediately in $H$ by a matching response event.

History $H'$ is an extension of history $H$ if $H$ is a prefix of $H'$.
History $H'$ is a \emph{completion} of history $H$ if $H'$ contains all the
events in $H$ and $H'$ is an extension of $H$, and each operation execution in $H'$ is complete.

$H|obj$ of history $H$ is the subsequence of all
invocation and response events in $H$ on object $obj$. If all invocation and response
events in a history $H$ have the same object name $obj$, then the $H|obj = H$.

Let $H$ be a complete history. We associate a time interval $I_{oe} = [t_0, t_1]$ with each
operation execution $oe$ = (INV, RSP) in $H$, where $t_0$ and $t_1$ are the points in time when INV and RSP happen.
Similarly, for an incomplete history, we denote the time interval $I_{oe}$ with respect to a pending
operation execution $oe$ = (INV, $null$) by $I_{oe} = [t_0, \infty]$.

Operation execution $oe_0$ \emph{precedes} operation execution $oe_1$ in $H$ if the response event of
$oe_0$ happens before the invocation event of $oe_1$ in $H$.
We say that $oe_0$ and $oe_1$ are \emph{concurrent} in $H$ if neither precedes the other.

A history is \emph{sequential} if its first event is an invocation event, and each invocation event, except
possibly the last one, is immediately followed by a matching response event.

A \emph{sequential specification} of an object is the set of all possible sequential histories
for that object.

A sequential history $S$ is \emph{valid}, if for each object $obj$, $S|obj$ is
in the sequential specification of $obj$.

\textbf{Linearization}.
A history $H$ \emph{linearizes} to a sequential history $S$, if and only if $S$ satisfies the
following conditions: (1) $S$ and any completion of $H$ have the same operation executions, (2) sequential history $S$ is
valid, and (3) there is a mapping from each time interval $I_{oe}$ to a time point $t_{oe} \in I_{oe}$, such
that the sequential history $S$ is obtained by sorting the operations in $H$ based on their $t_{oe}$ values.

A history is \emph{linearizable} if and only if $H$ linearizes to some sequential history $S$. In this case,
$S$ is called the \emph{linearization} of $H$. For each operation $opt$ in history $H$, we call time point $t_{oe}$, which is
defined as above, the \emph{linearization point} of $opt$. An object $obj$ is linearizable if every
history $H$ on $obj$ is linearizable.

\section{Base Objects}
In this section, we describe the two base objects, i.e, \emph{read-write register} and \emph{compare-and-swap} (CAS) objects, which will be used in our following discussion. Most implementations
of more sophisticated objects use them as the base objects in their implementations and
most modern architectures support either read-write registers and CAS objects\cite{itanium}\cite{weaver1994sparc}.

\textbf{Read-Write Register}.
An object that supports only \texttt{read()} and \texttt{write(x)} operations is called
a read-write register (or just \emph{register}). Operation \texttt{read()} returns the current state of register and
leaves the state unchanged. Operation \texttt{write(x)} changes the state of the register to $x$ and returns nothing.
If the set of states that can be stored in the register is unbounded then we say the register is \emph{unbounded register};
otherwise the register is \emph{bounded register}.

\textbf{CAS Object}. An object that supports \texttt{read()} and \texttt{CAS(x,y)} operations is called compare-and-swap (CAS) object.
Operation \texttt{read()} is the same as defined above. Operation \texttt{CAS(x,y)} changes the state of
the object if and only if the current state is equal to $x$ and then operation \texttt{CAS(x,y)} succeeds, and the state is changed
to $y$ and $true$ is returned. Otherwise, operation \texttt{CAS(x,y)} fails, the current state remains unchanged and
$false$ is returned.

\section{Adversary Models for Randomized Algorithms}

\textbf{Randomness}.
A randomized algorithm is an algorithm where processes are allowed to make random decisions for the next step
by calling a special operation called \emph{coin-flip operation}. We also say a process \emph{flips a coin}
when it calls this operation.

When a process flips a coin, it receives a random value $c$ from some arbitrary set $\Omega$
which is calle the \emph{coin-flip domain}. The process can then use this random value $c$ in its program for future decisions.

A vector $\overrightarrow{\rm c} = (c_0, c_1, c_2,...) \in \Omega^{\infty}$ is called a \emph{coin-flip vector}.
A history $H$ is said to \emph{observe} the coin-flip vector $\overrightarrow{\rm c}$ if
for an arbitrary integer $i \in [0, \infty)$, the $i$-th coin-flip operation in $H$ returns value $c_i \in \Omega$.

For a history $H$ that contains $k$ coin-flip operations, we use $H[k]$ to denote the prefix of $H$
that ends with the $k$-th invocation of a coin-flip operation. If fewer than $k$ coin-flip operation
executions occur during H, then $H[k]$ denotes $H$.

In the following discussion, we use operation \texttt{random($\Omega$)}, which is assumed
to be atomic, to return a value which is distributed uniformly at random over coin-flip domain
$\Omega = \{0, 1, 2,..., s-1\}$.

\textbf{Schedule}.
In the standard shared memory model, each process executes its program by applying
shared memory operations (\texttt{read()}, \texttt{write(x)}, \texttt{CAS(x,y)}, etc) on objects,
as determined by their program. Operation executions of concurrent processes
can be interleaved arbitrarily.

A \emph{schedule} with \emph{length} $k$ is represented by a sequence of process ids
$$p = (p_0, p_1, p_2,..., p_{k-1})$$ where $k \in [0, \infty)$ and for each $i \in [0, k-1]$,
$p_i \in \mathcal{P}$.

Let a schedule $p = (p_0, p_1, p_2,..., p_{k-1})$.
A history $H$ is said to \emph{observe} schedule $p$ if the number of events in $H$ is $k$,
and for each integer $i \in [0, k-1]$, the $i$-th event is applied by process $p_i$.

\textbf{Adversary}.
In randomized algorithm, the random choices processes make can influence the schedule.
To model the worst-case possible way that the system can be influenced by the random choices,
schedules are assumed to be generated by an adversarial scheduler, called the \emph{adversary}.

Mathematically, an adversary is defined as a mapping\cite{golab2011linearizable}:
\begin{center}
$\mathcal{A} :  \Omega^{\infty} \to \mathcal{P}^{\infty}$
\end{center}

Given an algorithm $\mathcal{M}$, an adversary $\mathcal{A}$, and
a coin-flip vector $\overrightarrow{\rm c} \in \Omega^{\infty}$,
a unique history $H_{\mathcal{M},\mathcal{A}, \overrightarrow{\rm c}}$ is generated, such that all
processes apply events as dictated by algorithm $\mathcal{M}$, and history
$H_{\mathcal{M}, \mathcal{A}, \overrightarrow{\rm c}}$
observes the schedule $\mathcal{A(\overrightarrow{\rm c})}$ and the coin flip vector $\overrightarrow{\rm c}$.

There are several adversary models with different strengths\cite{DBLP:journals/corr/cs-DS-0209014}.
In our thesis, we only conside the \emph{adaptive adversary}.

Informally, the adaptive adversary makes scheduling
decisions as follows: At any point, it can see the entire history up to that point.
This includes all coin-flip operations and their return values up to that point. Depending on this,
the adversary decides which process takes the next step.

Adversary $\mathcal{A}$ is \emph{adaptive for algorithm $\mathcal{M}$}\cite{golab2011linearizable}
if, for any two coin-flip
vectors $\overrightarrow{\rm c} \in \Omega^{\infty}$ and $\overrightarrow{\rm d} \in \Omega^{\infty}$ that have a common prefix
of length $k$ (i.e, the first $k$ elements of $\overrightarrow{\rm c}$ and $\overrightarrow{\rm d}$ are the same), then we have
$$H_{\mathcal{M},\mathcal{A}, \overrightarrow{\rm c}}[k+1] = H_{\mathcal{M},\mathcal{A}, \overrightarrow{\rm d}}[k+1]$$
In this case, we say adversary $\mathcal{A}$ is an \emph{adaptive adversary}.

From the above definition, we can see an adaptive adversary can not use
future coin flips to make current scheduling decisions.

\section{The Dynamic Task Alloction Problem}

% * \emph{define what is task? what is perform a task and insert a task?}

\subsection{Task}
Performing a task can be be any execution performed by a single
process in constant time and the tasks are assumed to satisfy the following three
properties\cite{georgiou2007all}:
\emph{Similarity}: The execution consume equal or comparable resources.
\emph{Independence}: One execution is independent of any of the others.
\emph{Idempotence}: Tasks could be performed more than one time.

For simplicity, in our thesis, \emph{tasks} are abstracted and defined as zero-valued shared registers
in an array, denoted as \texttt{tasks}$[ ]$.

The state of each register is either $0$ or $1$. When a register is in state $0$,
then we say a task is \emph{available}. When the
state of that register is flipped to $1$, we say that task is \emph{performed}.

Each array cell \texttt{tasks}$[i]$, where
integer $i \in [0, \infty)$ is defined as a \emph{memory location} (or just \emph{location}) and denoted as $M$.
We say there is an available task \emph{associated} with location $M$ (or at $M$)
if the register at location $M$ is in state $0$.
Otherwise, we say there is no task at $M$ if the register is in state $1$.

Each task has a unique identifier from set
$\mathcal{L} = [m] = \{\ell_0, \ell_1, \ell_2,..., \ell_m\}$, where $m \in [0, \infty)$.
A simple unique task identifier scheme is provided by\cite{alistarh2014dynamic}.

Informally, \emph{performing} a task is the execution that flipping the state of a register at some location from
$0$ to $1$. If the above value $m$ is fixed and the $m$ tasks are available initially,
and the execution ends when all tasks are performed, then we get the so-called \emph{static} version of task allocation problem,
i.e, write-all problem as follws: Given a zero-valued array of $m$ elements and $n$ processes,
write value $1$ into each array cell in the presence of adversity\cite{kanellakis1992efficient}.

In this thesis, we consider the \emph{dynamic} version of task allocation, i.e, Given $n$
asynchronous processes that cooperate to perform tasks, while tasks are \emph{inserted}
dynamically by the adversary during the execution, where \emph{inserting} a task is definied to be
the execution that flipping the state of a register from state $1$ back to $0$.
When the state of the register at location $M$
is flipped from $1$ to $0$, we also say a new task is \emph{inserted to locaton} $M$.

Since the state of the register at location $M$ can be flipped from $0$ to $1$ and from $1$ back to $0$
several times, thus we say one location $M$ can be associated with multiple tasks over time.

Now we specify the dynamic task alloction problem in terms of a type DTA, and the properties
that an implementation of type DTA must satisfy.

\subsection{The Type DTA}

The type DTA supports two types of operations, \texttt{DoTask()} and
\texttt{InsertTask(}$\ell$\texttt{)}, where $\ell$ is the task identifier.


\textbf{Operation DoTask()}.
The aim of operation \texttt{DoTask()} is to perform a task associated with location $M$ by calling
a special atomic operation \texttt{TryTask(}$M$\texttt{)} which is assumed to exist.

Task $\ell$ can be performed atomically by a process by calling \texttt{TryTask(}$M$\texttt{)} operation
with argument $M$ which is the memory location associated with task $\ell$. Out of several processes
calling \texttt{TryTask(}$M$\texttt{)}, only the first one receives notification $success$ and the identifier $\ell$ of
that task, while all the others only receive $failure$.

Every \texttt{DoTask()} operation may perform several \texttt{TryTask(}$M$\texttt{)}
operations. However, only one of them will succeed. Once one \texttt{TryTask(}$M$\texttt{)} succeeds, then
task identifier $\ell$ will be returned by \texttt{DoTask()}.
If there is no task could be performed, i.e, the state of each task is $1$,
then operation \texttt{DoTask()} returns $\perp$.


\textbf{Operation InsertTask($\ell$)}.
The goal of \texttt{InsertTask(}$\ell$\texttt{)} operation is to insert task $\ell$ to location $M$
also by calling a special atomic operation \texttt{PutTask(}$M,\ell$\texttt{)}.

Task $\ell$ can be
associated with memory location $M$ atomically by calling \texttt{PutTask(}$M,\ell$\texttt{)} operation.
\texttt{PutTask(}$M,\ell$\texttt{)} returns $success$ if task $\ell$ is associated with $M$,
and returns $failure$ if location $M$ was already associated with another task.

Several \texttt{PutTask(}$M,\ell$\texttt{)} operations may be executed but only one of them will succeed.
\texttt{PutTask(}$M,\ell$\texttt{)} fails if location $M$ has been associated with another task in state $1$.
Once one \texttt{TryTask(}$M$\texttt{)} succeeds, then task $\ell$ is available on location $M$ and $success$
notification is returned by \texttt{InsertTask(}$\ell$\texttt{)} operation.

Similarly, atomic operations \texttt{PutTask(}$M,\ell$\texttt{)} it assumed to be contained in \texttt{InsertTask(}$\ell$\texttt{)}
operation.

A task is performed if its identifier has been returned by a process after calling \texttt{DoTask()}.
A task $\ell$ is available at location $M$ if $success$ is returned by a process
after calling \texttt{InsertTask(}$\ell$\texttt{)}, but is not performed yet.

\textbf{Properties}.
An algorithm that accesses an instance of an object of type DTA must satisfy the following:

\emph{Validity}. If a \texttt{DoTask()} operation returns $\ell$, then before the
\texttt{DoTask()} operation, an \texttt{InsertTask(}$\ell$\texttt{)} was executed and returned $success$.

\emph{Uniqueness}. Each task is performed at most once, i.e, for each task $\ell$, at most one \texttt{DoTask()}
operation returns $\ell$.

In addition, the property that every inserted task is eventually done is also a desired progress property
of the implementation of type DTA.
