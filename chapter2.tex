
%--------------------CHAPTER 2-----------------------------
\chapter{Model of Computation and Definitions}
In this chapter, we will describe our model of computation and give the definitions, which are based on Herlihy
and Wing's \cite{Herlihy:1990:LCC:78969.78972} and Golab, Hadzilacos and Woelfel's \cite{InProc-GHHW2007a}.

The computational model we consider is the standard asynchronous shared memory model with a set of $n$ processes,
denoted as $\{p_1, p_2,...,p_n\}$ , where up to $n-1$ processes may fail by crashing.

\textbf{Type and Object}.
A \emph{type} $\tau$ is defined as an automaton as follows \cite{InProc-GHHW2007a},
$$\tau = (\mathcal{S}, s_{init},\mathcal{O},\mathcal{R} ,\delta )$$

where $\mathcal{S}$ is a set of states, $s_{init} \in \mathcal{S}$ is the initial state, $\mathcal{O}$ is a set of
operation types, $\mathcal{R}$ is the set of responses, and
$\delta :\mathcal{S} \times \mathcal{O} \to \mathcal{S} \times \mathcal{R}$ is a state transition mapping.

An \emph{object} is an implementation of a type. For each type $\tau$, the transition mapping $\delta$ captures the
behaviour of objects of type $\tau$, in the absence of concurreny,
as follows: if a process applies an operation of type $opt$ to an object of type $\tau$ which is in state $s$, the object
may return to the process a response $rsp$ and change its states to $s'$ if and only if $(s', rsp) \in \delta(s, opt)$.

An object that supports only \texttt{read()} and \texttt{write(x)} operations is called a \emph{read/write register}
(or just \emph{register}). Operation \texttt{read()} returns the current state of the register and leaves the state unchanged.
Operation \texttt{write(x)} changes the state of the register to $x$.

An object that supports \texttt{read()} and \texttt{CAS(x,y)} operations is called \emph{compare-and-swap} (CAS) object.
Operation \texttt{read()} is the same as defined above. Operation \texttt{CAS(x,y)} changes the state of
the object if and only if the current state is equal to $x$ and then operation \texttt{CAS(x,y)} succeeds, and the state is changed
to $y$ and $true$ is returned. Otherwise, operation \texttt{CAS(x,y)} fails, the current state remains unchanged and
$false$ is returned.

\textbf{History}
A \emph{history} $H$, obtained by processes executing
operations on objects, is a sequence of \emph{invocation}
and \emph{response} events.

An invocation event is a 5-tuple,
\begin{center}
INV = $(invocation, p, obj, opt, t)$
\end{center}
where $invocation$ is the event type, $p$ is the process executing the operation, $obj$ is the object on which the operation
is executed, $opt$ is the operation type and $t$ is the \emph{time} when INV happens which is defined
as the position of event INV in history $H$.

A response event is also a 5-tuple,
\begin{center}
RSP = $(response, p, obj, rsp, t)$
\end{center}
where $response$ is the event type, $p$ is the process receiving response $rsp$ from an oeration on object $obj$ and $t$
is the time when RSP happens which is defined as the position of event RSP in history $H$.

Response event $(response, p_j, obj_q, rsp, t_1)$ \emph{matches} invocation event $(invocation, p_i, obj_p, opt, t_0)$
in history $H$, if the two events are applied by the same process to the same object, i.e, $i = j$ and $p = q$.
In this case, the response event is also called the $matching$ response of the invocation event.

An \emph{operation execution} in $H$ is a pair $oe$ = (INV, RSP) consisting of an invocation event INV
and the next matching response event RSP, or just an invocation event INV, denoted as $oe$ = (INV, $null$).
In the latter case, we say the operation execution is \emph{pending}. In the former case, we say the operation
execution is \emph{complete}. A history $H$ is \emph{complete} if all operation executions in $H$ are \emph{complete},
otherwise, it is \emph{incomplete}.

History $H'$ is an extension of history $H$ if $H$ is a prefix of $H'$.
History $H'$ is a \emph{completion} of history $H$ if $H'$ contains all the
events in $H$ and $H'$ is an extension of $H$, and each operation execution in $H'$ is complete.

$H|obj$ of history $H$ is the subsequence of all
invocation and response events in $H$ on object $obj$. If all invocation and response
events in a history $H$ have the same object name $obj$, then the $H|obj = H$.

Let $H$ be a complete history. We associate a time interval $I_{oe} = [t_0, t_1]$ with each
operation execution $oe$ = (INV, RSP) in $H$, where $t_0$ and $t_1$ are the points in time when INV and RSP happen.
Similarly, for an incomplete history, we denote the time interval $I_{oe}$ with respect to a pending
operation execution $oe$ = (INV, $null$) by $I_{oe} = [t_0, \infty]$.

Operation execution $oe_0$ \emph{precedes} operation execution $oe_1$ in $H$ if the response event of
$oe_0$ happens before the invocation event of $oe_1$ in $H$.
We say that $oe_0$ and $oe_1$ are \emph{concurrent} in $H$ if neither precedes the other.

A history is \emph{sequential} if its first event is an invocation event, and each invocation event, except
possibly the last one, is immediately followed by a matching response event.

\textbf{Linearization.}
A history $H$ \emph{linearizes} to a sequential history $S$, if and only if $S$ satisfies the
following conditions: (1) $S$ and any completion of $H$ have the same operation executions, (2) sequential history $S$ is
valid, and (3) there is a mapping from each time interval $I_{oe}$ to a time point $t_{oe} \in I_{oe}$, such
that the sequential history $S$ is obtained by sorting the operations in $H$ based on their $t_{oe}$ values.

A history is \emph{linearizable} if and only if $H$ linearizes to some sequential history $S$. In this case,
$S$ is called the \emph{linearization} of $H$. For each operation $opt$ in history $H$, we call time point $t_{oe}$, which is
defined as above, the \emph{linearization point} of $opt$. An object $obj$ is linearizable if every history $H$ on $obj$ is linearizable.

\textbf{Randomness}.
A process can execute local coin flip operations that return integer values distributed
uniformly at random from an arbitrary finite set of integers. In the following discussion, we use method
\texttt{random(s)} to return a value which is distributed uniformly at random from set $\{0, 1, 2,..., s-1\}$.

\textbf{Adversary}.
In the standard shared memory model, each process executes its program by applying
shared memory operations (\texttt{read()}, \texttt{write(x)}, \texttt{CAS(x,y)}, etc) on objects,
as determined by their program. Operation executions of concurrent processes
can be interleaved arbitrarily.

For deterministic shared memory algorithms, we analysize their performance via the standard  worst-case analysis assuming that
this interleaving occurs in the ``worst" possible way. However, in the analysis of randomized algorithms, we use an
$adversary$ model to describe how the interleaving of executions can be influenced by random choices
made by processes.

There are several adversary models with different strengths
\cite{DBLP:journals/corr/cs-DS-0209014},
The most pessimistic reasonable assumption is modeled by the \emph{strong adaptive adversary}. We analyze
our randomized shared memory algorithm under the assumption of a strong adaptive adversary, i.e, at any
point of time, it can see the entire past history and knows the states of all processes and decides on
the schedule accordingly.
