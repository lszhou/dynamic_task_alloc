%--------------------CHAPTER 2-----------------------------
\chapter{Model of Computation and Definitions}
\section{Asynchronous Shared Memory Model}
In this chapter, we will describe our model of computation and give the definitions, which are based on Herlihy
and Wing's \cite{Herlihy:1990:LCC:78969.78972} and Golab, Higham and Woelfel's \cite{golab2011linearizable}.

The computational model we consider is the standard asynchronous shared memory model with a set $\mathcal{P}$
of $n$ processes, denoted as $\mathcal{P} = [p] =\{0, 1, 2,..., n-1\}$ , where up to $n-1$ processes may fail by crashing.
A process may crash at any moment during the computation and once crashed it does not restart,
and does not perform any further actions.

\textbf{Type and Object}.
A \emph{type} $\tau$ is defined as an automaton as follows \cite{InProc-GHHW2007a},
$$\tau = (\mathcal{S}, s_{init},\mathcal{O},\mathcal{R} ,\delta )$$

where $\mathcal{S}$ is a set of states, $s_{init} \in \mathcal{S}$ is the initial state, $\mathcal{O}$ is a set of
operations, $\mathcal{R}$ is the set of responses, and
$\delta :\mathcal{S} \times \mathcal{O} \to \mathcal{S} \times \mathcal{R}$ is a state transition mapping.

An \emph{object} is an implementation of a type. For each type $\tau$, the transition mapping $\delta$ captures the
behaviour of objects of type $\tau$, in the absence of concurreny,
as follows: if a process applies an operation $opt$ to an object of type $\tau$ which is in state $s$, the object
may return to the process a response $rsp$ and change its states to $s'$ if and only if $(s', rsp) \in \delta(s, opt)$.

\textbf{History}.
A \emph{history} $H$, obtained by processes executing
operations on objects, is a sequence of invocation
and response events.

An invocation event is a 5-tuple,
\begin{center}
INV = $(invocation, p, obj, opt, t)$
\end{center}
where $invocation$ is the event type, $p$ is the process executing the operation, $obj$ is the object on which the operation
is executed, $opt$ is the operation and $t$ is the \emph{time} when INV happens which is defined
as the position of event INV in history $H$. We also say the event INV is the invocation event of operation $opt$.

A response event is also a 5-tuple,
\begin{center}
RSP = $(response, p, obj, rsp, t)$
\end{center}
where $response$ is the event type, $p$ is the process receiving response $rsp$ from an operation on object $obj$ and $t$
is the time when RSP happens which is defined as the position of event RSP in history $H$.

In the following discussion, we suppose in a history $H$, the situation that an invocation event
$(invocation, p_i, obj_p, opt_0, t_0)$ is followed immediately by another invocation event\\
$(invocation, p_j, obj_q, opt_1, t_1)$ where $i = j$ and $p = q$ will not happen.

We say response event $(response, p_j, obj_q, rsp, t_1)$ \emph{matches} invocation event\\
$(invocation, p_i, obj_p, opt, t_0)$
in history $H$, if the two events are applied by the same process to the same object, i.e, $i = j$ and $p = q$.
In this case, the response event is also called the $matching$ response of the invocation event.

An \emph{operation execution} in $H$ is a pair $oe$ = (INV, RSP) consisting of an invocation event INV
and its matching response event RSP, or just an invocation event INV with no matching response event,
denoted as $oe$ = (INV, $null$).

In the latter case, we say the operation execution is \emph{pending}. In the former case, we say the operation
execution is \emph{complete}.

A history $H$ is \emph{complete} if all operation executions in $H$ are \emph{complete}.
Otherwise, it is \emph{incomplete}.

If events INV and RSP are applied by process $p$, then we say
operation execution $oe$ = (INV, RSP) is \emph{performed} by process $p$. Thus, two operation
executions performed by the same process on the same project will not interleave in a history $H$.

We say that an operation $opt$ is \emph{atomic} in history $H$, if $opt$'s invocation event is either the last event in
$H$, or else is followed immediately in $H$ by a matching response event.

If history $H$ is a prefix of history $H'$, then we say history $H'$ is an extension of $H$.
History $H'$ is a \emph{completion} of history $H$ if $H'$ contains all events in $H$ and
$H'$ is an extension of $H$, and each operation execution in $H'$ is complete.

$H|obj$ of history $H$ is the subsequence of all
invocation and response events in $H$ on object $obj$. If all invocation and response
events in a history $H$ have the same object name $obj$, then the $H|obj = H$.

Let $H$ be a complete history. We associate a time interval $I_{oe} = [t_0, t_1]$ with each
operation execution $oe$ = (INV, RSP) in $H$, where $t_0$ and $t_1$ are the points in time when INV and RSP happen.
Similarly, for an incomplete history, we denote the time interval $I_{oe}$ with respect to a pending
operation execution $oe$ = (INV, $null$) by $I_{oe} = [t_0, \infty]$.

Operation execution $oe_0$ \emph{precedes} operation execution $oe_1$ in $H$ if the response event of
$oe_0$ happens before the invocation event of $oe_1$ in $H$.
We say that $oe_0$ and $oe_1$ are \emph{concurrent} in $H$ if neither precedes the other.

A history is \emph{sequential} if its first event is an invocation event, and each invocation event, except
possibly the last one, is immediately followed by a matching response event.

A \emph{sequential specification} of an object is the set of all possible sequential histories
for that object.

A sequential history $S$ is \emph{valid}, if for each object $obj$, $S|obj$ is
in the sequential specification of $obj$.

\textbf{Linearization}.
A history $H$ \emph{linearizes} to a sequential history $S$, if and only if $S$ satisfies the
following conditions:
\begin{itemize}[leftmargin=1cm]
  \item $S$ and any completion of $H$ have the same operation executions,
  \item sequential history $S$ is valid, and
  \item there is a mapping from each time interval $I_{oe}$ to a time point $t_{oe} \in I_{oe}$, such
  that the sequential history $S$ is obtained by sorting the operations in $H$ based on their $t_{oe}$ values.
\end{itemize}

A history is \emph{linearizable} if and only if $H$ linearizes to some sequential history $S$. In this case,
$S$ is called the \emph{linearization} of $H$. For each operation $opt$ in history $H$, we call time point $t_{oe}$, which is
defined as above, the \emph{linearization point} of $opt$. An object $obj$ is linearizable if every
history $H$ on $obj$ is linearizable.

\section{Base Objects}
In this section, we describe the two base objects, i.e, \emph{read-write register} and \emph{compare-and-swap} (CAS)
objects, which will be used in our following discussion. Most implementations
of more sophisticated objects use them as the base objects in their implementations and
most modern architectures support either read-write registers and CAS objects \cite{itanium} \cite{weaver1994sparc}.

\textbf{Read-Write Register}.
An object that supports only \texttt{read()} and \texttt{write(x)} operations is called
a read-write register (or just \emph{register}).

Operation \texttt{read()} returns the current state of register and
leaves the state unchanged. Operation \texttt{write(x)} changes the state of the register to $x$ and returns nothing.
If the set of states that can be stored in the register is unbounded then we say the register is \emph{unbounded register};
otherwise the register is \emph{bounded register}.

\textbf{CAS Object}. An object that supports \texttt{read()} and \texttt{CAS(x,y)} operations is called compare-and-swap (CAS) object.

Operation \texttt{read()} returns the current state of CAS object, and
leaves the state unchanged, while operation \texttt{CAS(x,y)} changes the state of
the object if and only if the current state is equal to $x$ and then operation \texttt{CAS(x,y)} succeeds, and the state is changed
to $y$ and $true$ is returned. Otherwise, operation \texttt{CAS(x,y)} fails, the current state remains unchanged and
$false$ is returned.

\section{Adversary Models for Randomized Algorithms}

\textbf{Randomness}.
A randomized algorithm is an algorithm where processes are allowed to make random decisions for future steps
by calling a special operation called \emph{coin-flip operation}. We also say a process \emph{flips a coin}
when it calls this operation.

When a process flips a coin, it receives a random value $c$ from some arbitrary countable set $\Omega$,
which is the \emph{coin-flip domain}. The process can then use this random value $c$ in its program for future decisions.

A vector $\overrightarrow{\rm c} = (c_0, c_1, c_2,...) \in \Omega^{\infty}$ is called a \emph{coin-flip vector}.
A history $H$ is said to \emph{observe} the coin-flip vector $\overrightarrow{\rm c}$ if
for any integer $i \in \{0, 1, 2, ...\}$, the $i$-th coin-flip operation in $H$ returns value $c_i \in \Omega$.

For a history $H$ that contains $k$ coin-flip operations, we use $H[k]$ to denote the prefix of $H$
that ends with the $k$-th invocation of a coin-flip operation. If fewer than $k$ coin-flips occur during H, then $H[k]$ denotes $H$.

\textbf{Schedule}.
In the standard shared memory model, each process executes its program by applying
shared memory operations (\texttt{read()}, \texttt{write(x)}, \texttt{CAS(x,y)}, etc) on objects,
as determined by their program. Operation executions of concurrent processes
can be interleaved arbitrarily.

A \emph{schedule} with \emph{length} $k$ is represented by a sequence of process IDs
$$p = (p_0, p_1, p_2,..., p_{k-1})$$ where $k \in \{1, 2, 3, ...\}$ and for each $i \in \{0, 1, ..., k-1\}$,
$p_i \in \mathcal{P}$.

Consider a schedule $p = (p_0, p_1, p_2,..., p_{k-1})$.
A history $H$ is said to \emph{observe} schedule $p$ if the number of events in $H$ is $k$,
and for each integer $i \in \{0, 1, ..., k-1\}$, the $i$-th event is applied by process $p_i$.

\textbf{Adversary}.
In a randomized algorithm, the random choices processes make can influence the schedule.
To model the worst possible way that the system can be influenced by the random choices,
schedules are assumed to be generated by an adversarial scheduler, called the \emph{adversary}.

Mathematically, an adversary is defined as a mapping \cite{golab2011linearizable}:
\begin{center}
$\mathcal{A} :  \Omega^{\infty} \to \mathcal{P}^{\infty}$
\end{center}

Given an algorithm $\mathcal{M}$, an adversary $\mathcal{A}$, and
a coin-flip vector $\overrightarrow{\rm c} \in \Omega^{\infty}$,
a unique history $H_{\mathcal{M},\mathcal{A}, \overrightarrow{\rm c}}$ is generated, such that all
processes apply events as dictated by algorithm $\mathcal{M}$, and history
$H_{\mathcal{M}, \mathcal{A}, \overrightarrow{\rm c}}$
observes the schedule $\mathcal{A(\overrightarrow{\rm c})}$ and the coin flip vector $\overrightarrow{\rm c}$.

There are several adversary models with different strengths \cite{DBLP:journals/corr/cs-DS-0209014}.
In our thesis, we only conside the \emph{adaptive adversary}.

Informally, the adaptive adversary makes scheduling
decisions as follows: At any point, it can see the entire history up to that point.
This includes all coin-flip operations and their return values up to that point. Depending on this,
the adversary decides which process takes the next step.

Adversary $\mathcal{A}$ is \emph{adaptive for algorithm $\mathcal{M}$} \cite{golab2011linearizable}
if, for any two coin-flip vectors $\overrightarrow{\rm c} \in \Omega^{\infty}$ and $\overrightarrow{\rm d} \in \Omega^{\infty}$ that have a common prefix
of length $k$ (i.e, the first $k$ elements of $\overrightarrow{\rm c}$ and $\overrightarrow{\rm d}$ are the same), then we have
$$H_{\mathcal{M},\mathcal{A}, \overrightarrow{\rm c}}[k+1] = H_{\mathcal{M},\mathcal{A}, \overrightarrow{\rm d}}[k+1]$$
In this case, we say adversary $\mathcal{A}$ is an \emph{adaptive adversary}.

From the above definition, we can see an adaptive adversary cannot use
future coin flips to make current scheduling decisions.

\section{The Dynamic Task Alloction Problem}

\subsection{Task}

A \emph{task} is a computation which is assumed to be performed by a single process in constant time\cite{georgiou2007all}.
In this thesis, we consider a finite or infinite set of tasks, denoted as $\mathcal{L} = [m] = \{0, 1, ..., m-1\}$,
where $m \in \{0, 1, 2, ...\}$.

We assume that each task $\ell \in \mathcal{L}$ to be performed is associated with a \emph{location} $M$
in the data structure. Over time, one location can be associated with multiple tasks.

In this section, we are going to specify the dynamic task alloction problem in terms of a type DTA which
suports two types of operations \texttt{DoTask} and \texttt{InsertTask}, and the properties
that an implementation of type DTA must satisfy. But before that, we firstly fix an interface by which processes could
perform a task, or insert a new task to data structure.

\textbf{Operation TryTask($M$)}.
A process can perform a task $\ell$ atomically by calling a special
\texttt{TryTask(}$M$\texttt{)} operation where $M$ is a location which task $\ell$
is associated with.


If the location $M$ is associated with a task $\ell$, then notification \emph{success} will be returned
by \texttt{TryTask(}$M$\texttt{)}.
Otherwise, if there is no task associated with location $M$, then \emph{failure} will be
returned.

\textbf{Operation PutTask($M,\ell$)}.
A process can associate a task $\ell$ with a location $M$ in the data structure
atomically by calling a special \texttt{PutTask(}$M,\ell$\texttt{)} operation.

If location $M$ is not associated with any other task, then \texttt{PutTask(}$M,\ell$\texttt{)} will return \emph{success}.
Otherwise, if location $M$ is already associated with another task $\ell'$, then $failure$ is returned
by \texttt{PutTask(}$M,\ell$\texttt{)} call.

\subsection{The Type DTA}

The type DTA supports two types of operations. The \texttt{DoTask()} operation performes a task
and returns the identifier of that task, while the \texttt{InsertTask(}$\ell$\texttt{)} operation associates task
$\ell$ with a location in the data structure to be performed. Now we describe the sequential specification
as follows.

\textbf{Operation DoTask()}.
The aim of operation \texttt{DoTask()} is to find a location $M$ which is associated with a task
$\ell$ in the data structure and then perform task $\ell$ by calling the atomic operation \texttt{TryTask(}$M$\texttt{)}.

Every \texttt{DoTask()} operation may perform several \texttt{TryTask} operation with different locations as the
arguments. Once a \texttt{TryTask} operation succeeds, \texttt{DoTask()} terminates and
the task identifier $\ell$ is returned. Otherwise, \texttt{DoTask()} never terminates and keeps calling
\texttt{TryTask(}$M$\texttt{)} repeatedly. If there is no task in the data structure,
then \texttt{DoTask()} returns $\perp$.

A task $\ell$ is said to be \emph{performed} by a process if the process has completed a \texttt{DoTask()} call
which returns the task identifier $\ell$.

\textbf{Operation InsertTask($\ell$)}.
The goal of \texttt{InsertTask(}$\ell$\texttt{)} operation is to find a location $M$ in the data structure and
associates task $\ell$ with $M$ by calling the atomic operation \texttt{PutTask(}$M,\ell$\texttt{)}.

Every \texttt{InsertTask(}$\ell$\texttt{)} operation may perform several \texttt{PutTask} operations with
different locations as the arguments. Operation \texttt{InsertTask(}$\ell$\texttt{)} terminates once one of the
\texttt{PutTask} operations succeeds and then location $M$ will be returned by \texttt{InsertTask(}$\ell$\texttt{)}.
Otherwise, \texttt{InsertTask(}$\ell$\texttt{)} never terminates and keeps
calling \texttt{PutTask} operations repeatedly.

We say task $\ell$ is associated with a location $M$ or inserted into the data structure or task $\ell$
is \emph{available} to perform if a process has completed the \texttt{InsertTask(}$\ell$\texttt{)} call and the location
$M$ is returned, but task $\ell$ has not been performed yet.

% \textbf{Properties}.
% An algorithm that accesses an instance of an object of type DTA must satisfy the following:
%
% \emph{Validity}. If a \texttt{DoTask()} operation returns $\ell$, then before the
% \texttt{DoTask()} operation, an \texttt{InsertTask(}$\ell$\texttt{)} was executed and returned $success$.
%
% \emph{Uniqueness}. Each task is performed at most once, i.e, for each task $\ell$, at most one \texttt{DoTask()}
% operation returns $\ell$.
%
% In addition, the property that every inserted task is eventually performed is also a desired progress property
% of the implementation of type DTA.


%
% For simplicity, in our thesis, we consider the tasks as follows:
%
% Give an array of registers which is denoted as \texttt{arr}$[]$. The state of each register is either $\perp$ or $0$ or $1$.
% A task is the computation that flipping the state of a register from $0$ to $1$. We denote the computation
% flipping the state of register \texttt{arr}$[i]$ from $0$ to $1$
% as $\ell_i$, , where integer $i \in \{0, 1, 2, ...\}$ and $\ell_i \in \mathcal{L}$
%
% If the above value $m$ is fixed and all $m$ registers are all initialized with value $0$ and the execution ends
% when all registers are flipped to $1$, then we get the so-called \emph{static} version of task allocation problem,
% i.e, write-all problem as follws: Given a zero-valued array of $m$ elements and $n$ processes,
% write value $1$ into each array cell in the presence of adversity \cite{kanellakis1992efficient}.
%
% Each array cell \texttt{arr}$[i]$ is defined as a \emph{memory location} (or just \emph{location}) and denoted as $M$.
% We say there is a task available at location $M$ if the register \texttt{arr}$[i]$ is in state $0$.
% When there exists a register in \texttt{arr} is in state $0$, then we say a task is \emph{available}.
% When the state of that register is flipped to $1$, then we say the task is \emph{performed}.
% If the state of register \texttt{arr}$[i]$ is $\perp$, then we say, there is no task there.
%
% In this thesis, we consider the \emph{dynamic} version of task allocation, i.e, Given $n$
% asynchronous processes that cooperate to perform tasks, while tasks are \emph{inserted}
% dynamically by the adversary during the execution, where \emph{inserting} a task is definied to be
% the execution that flipping the state of a register from its initial state $\perp$ to $0$.
% When the state of the register at location $M$ is flipped from $\perp$ to $0$, we also say a new task is \emph{inserted to locaton} $M$.
