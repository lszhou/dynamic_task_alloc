

%--------------------CHAPTER 2-----------------------------
\chapter{Model of Computation and Definitions}
\section{Asynchronous Shared Memory Model}
In this chapter, we will describe our model of computation and give the definitions, which are based on Herlihy
and Wing's \cite{Herlihy:1990:LCC:78969.78972} and Golab, Higham and Woelfel's \cite{golab2011linearizable}.

The computational model we consider is the standard asynchronous shared memory model with a set $\mathcal{P}$
of $n$ processes, denoted as $\mathcal{P} = [p] =\{0, 1, 2,..., n-1\}$ , where up to $n-1$ processes may fail by crashing.
A process may crash at any moment during the computation and once crashed it does not restart,
and does not perform any further actions.

\textbf{Type and Object}.
A \emph{type} $\tau$ is defined as an automaton as follows \cite{InProc-GHHW2007a},
$$\tau = (\mathcal{S}, s_{init},\mathcal{O},\mathcal{R} ,\delta )$$

where $\mathcal{S}$ is a set of states, $s_{init} \in \mathcal{S}$ is the initial state, $\mathcal{O}$ is a set of
operations, $\mathcal{R}$ is the set of responses, and
$\delta :\mathcal{S} \times \mathcal{O} \to \mathcal{S} \times \mathcal{R}$ is a state transition mapping.

An \emph{object} is an implementation of a type. For each type $\tau$, the transition mapping $\delta$ captures the
behaviour of objects of type $\tau$, in the absence of concurreny,
as follows: if a process applies an operation $opt$ to an object of type $\tau$ which is in state $s$, the object
may return to the process a response $rsp$ and change its states to $s'$ if and only if $(s', rsp) \in \delta(s, opt)$.

\textbf{History}.
A \emph{history} $H$, obtained by processes executing
operations on objects, is a sequence of invocation
and response events.

An invocation event is a 5-tuple,
\begin{center}
INV = $(invocation, p, obj, opt, t)$
\end{center}
where $invocation$ is the event type, $p$ is the process executing the operation, $obj$ is the object on which the operation
is executed, $opt$ is the operation and $t$ is the \emph{time} when INV happens which is defined
as the position of event INV in history $H$. We also say the event INV is the invocation event of operation $opt$.

A response event is also a 5-tuple,
\begin{center}
RSP = $(response, p, obj, rsp, t)$
\end{center}
where $response$ is the event type, $p$ is the process receiving response $rsp$ from an oeration on object $obj$ and $t$
is the time when RSP happens which is defined as the position of event RSP in history $H$.

In the following discussion, we suppose in a history $H$, the situation that an invocation event
$(invocation, p_i, obj_p, opt_0, t_0)$ is followed immediately by another invocation event\\
$(invocation, p_j, obj_q, opt_1, t_1)$ where $i = j$ and $p = q$ will not happen.

Response event $(response, p_j, obj_q, rsp, t_1)$ \emph{matches} invocation event $(invocation, p_i, obj_p, opt, t_0)$
in history $H$, if the two events are applied by the same process to the same object, i.e, $i = j$ and $p = q$.
In this case, the response event is also called the $matching$ response of the invocation event.

An \emph{operation execution} in $H$ is a pair $oe$ = (INV, RSP) consisting of an invocation event INV
and its matching response event RSP, or just an invocation event INV with no matching response event,
denoted as $oe$ = (INV, $null$).
In the latter case, we say the operation execution is \emph{pending}. In the former case, we say the operation
execution is \emph{complete}. A history $H$ is \emph{complete} if all operation executions in $H$ are \emph{complete},
otherwise, it is \emph{incomplete}. If events INV and RSP are applied by process $p$, then we say
operation execution $oe$ = (INV, RSP) is \emph{performed} by process $p$. Thus, two operation
executions performed by the same process on the same project will not interleave in a history $H$.

We say that an operation $opt$ is \emph{atomic} in history $H$, if $opt$'s invocation event is either the last event in
$H$, or else is followed immediately in $H$ by a matching response event.

History $H'$ is an extension of history $H$ if $H$ is a prefix of $H'$.
History $H'$ is a \emph{completion} of history $H$ if $H'$ contains all the
events in $H$ and $H'$ is an extension of $H$, and each operation execution in $H'$ is complete.

$H|obj$ of history $H$ is the subsequence of all
invocation and response events in $H$ on object $obj$. If all invocation and response
events in a history $H$ have the same object name $obj$, then the $H|obj = H$.

Let $H$ be a complete history. We associate a time interval $I_{oe} = [t_0, t_1]$ with each
operation execution $oe$ = (INV, RSP) in $H$, where $t_0$ and $t_1$ are the points in time when INV and RSP happen.
Similarly, for an incomplete history, we denote the time interval $I_{oe}$ with respect to a pending
operation execution $oe$ = (INV, $null$) by $I_{oe} = [t_0, \infty]$.

Operation execution $oe_0$ \emph{precedes} operation execution $oe_1$ in $H$ if the response event of
$oe_0$ happens before the invocation event of $oe_1$ in $H$.
We say that $oe_0$ and $oe_1$ are \emph{concurrent} in $H$ if neither precedes the other.

A history is \emph{sequential} if its first event is an invocation event, and each invocation event, except
possibly the last one, is immediately followed by a matching response event.

A \emph{sequential specification} of an object is the set of all possible sequential histories
for that object.

A sequential history $S$ is \emph{valid}, if for each object $obj$, $S|obj$ is
in the sequential specification of $obj$.

\textbf{Linearization}.
A history $H$ \emph{linearizes} to a sequential history $S$, if and only if $S$ satisfies the
following conditions: (1) $S$ and any completion of $H$ have the same operation executions, (2) sequential history $S$ is
valid, and (3) there is a mapping from each time interval $I_{oe}$ to a time point $t_{oe} \in I_{oe}$, such
that the sequential history $S$ is obtained by sorting the operations in $H$ based on their $t_{oe}$ values.

A history is \emph{linearizable} if and only if $H$ linearizes to some sequential history $S$. In this case,
$S$ is called the \emph{linearization} of $H$. For each operation $opt$ in history $H$, we call time point $t_{oe}$, which is
defined as above, the \emph{linearization point} of $opt$. An object $obj$ is linearizable if every
history $H$ on $obj$ is linearizable.

\section{Base Objects}
In this section, we describe the two base objects, i.e, \emph{read-write register} and \emph{compare-and-swap} (CAS)
objects, which will be used in our following discussion. Most implementations
of more sophisticated objects use them as the base objects in their implementations and
most modern architectures support either read-write registers and CAS objects \cite{itanium} \cite{weaver1994sparc}.

\textbf{Read-Write Register}.
An object that supports only \texttt{read()} and \texttt{write(x)} operations is called
a read-write register (or just \emph{register}). Operation \texttt{read()} returns the current state of register and
leaves the state unchanged. Operation \texttt{write(x)} changes the state of the register to $x$ and returns nothing.
If the set of states that can be stored in the register is unbounded then we say the register is \emph{unbounded register};
otherwise the register is \emph{bounded register}.

\textbf{CAS Object}. An object that supports \texttt{read()} and \texttt{CAS(x,y)} operations is called compare-and-swap (CAS) object.
Operation \texttt{read()} is the same as defined above. Operation \texttt{CAS(x,y)} changes the state of
the object if and only if the current state is equal to $x$ and then operation \texttt{CAS(x,y)} succeeds, and the state is changed
to $y$ and $true$ is returned. Otherwise, operation \texttt{CAS(x,y)} fails, the current state remains unchanged and
$false$ is returned.

\section{Adversary Models for Randomized Algorithms}

\textbf{Randomness}.
A randomized algorithm is an algorithm where processes are allowed to make random decisions for future steps
by calling a special operation called \emph{coin-flip operation}. We also say a process \emph{flips a coin}
when it calls this operation.

When a process flips a coin, it receives a random value $c$ from some arbitrary countable set $\Omega$,
which is the \emph{coin-flip domain}. The process can then use this random value $c$ in its program for future decisions.

A vector $\overrightarrow{\rm c} = (c_0, c_1, c_2,...) \in \Omega^{\infty}$ is called a \emph{coin-flip vector}.
A history $H$ is said to \emph{observe} the coin-flip vector $\overrightarrow{\rm c}$ if
for any integer $i \in \{0, 1, 2, ...\}$, the $i$-th coin-flip operation in $H$ returns value $c_i \in \Omega$.

For a history $H$ that contains $k$ coin-flip operations, we use $H[k]$ to denote the prefix of $H$
that ends with the $k$-th invocation of a coin-flip operation. If fewer than $k$ coin-flips occur during H, then $H[k]$ denotes $H$.

% % put this paragraph later when using it
% In the following discussion, we use operation \texttt{random($\Omega$)}, which is assumed
% to be atomic, to return a value which is distributed uniformly at random over coin-flip domain
% $\Omega = \{0, 1, 2,..., s-1\}$.

\textbf{Schedule}.
In the standard shared memory model, each process executes its program by applying
shared memory operations (\texttt{read()}, \texttt{write(x)}, \texttt{CAS(x,y)}, etc) on objects,
as determined by their program. Operation executions of concurrent processes
can be interleaved arbitrarily.

A \emph{schedule} with \emph{length} $k$ is represented by a sequence of process IDs
$$p = (p_0, p_1, p_2,..., p_{k-1})$$ where $k \in \{1, 2, 3, ...\}$ and for each $i \in \{0, 1, ..., k-1\}$,
$p_i \in \mathcal{P}$.

Consider a schedule $p = (p_0, p_1, p_2,..., p_{k-1})$.
A history $H$ is said to \emph{observe} schedule $p$ if the number of events in $H$ is $k$,
and for each integer $i \in \{0, 1, ..., k-1\}$, the $i$-th event is applied by process $p_i$.

\textbf{Adversary}.
In a randomized algorithm, the random choices processes make can influence the schedule.
To model the worst possible way that the system can be influenced by the random choices,
schedules are assumed to be generated by an adversarial scheduler, called the \emph{adversary}.

Mathematically, an adversary is defined as a mapping \cite{golab2011linearizable}:
\begin{center}
$\mathcal{A} :  \Omega^{\infty} \to \mathcal{P}^{\infty}$
\end{center}

Given an algorithm $\mathcal{M}$, an adversary $\mathcal{A}$, and
a coin-flip vector $\overrightarrow{\rm c} \in \Omega^{\infty}$,
a unique history $H_{\mathcal{M},\mathcal{A}, \overrightarrow{\rm c}}$ is generated, such that all
processes apply events as dictated by algorithm $\mathcal{M}$, and history
$H_{\mathcal{M}, \mathcal{A}, \overrightarrow{\rm c}}$
observes the schedule $\mathcal{A(\overrightarrow{\rm c})}$ and the coin flip vector $\overrightarrow{\rm c}$.

There are several adversary models with different strengths \cite{DBLP:journals/corr/cs-DS-0209014}.
In our thesis, we only conside the \emph{adaptive adversary}.

Informally, the adaptive adversary makes scheduling
decisions as follows: At any point, it can see the entire history up to that point.
This includes all coin-flip operations and their return values up to that point. Depending on this,
the adversary decides which process takes the next step.

Adversary $\mathcal{A}$ is \emph{adaptive for algorithm $\mathcal{M}$} \cite{golab2011linearizable}
if, for any two coin-flip
vectors $\overrightarrow{\rm c} \in \Omega^{\infty}$ and $\overrightarrow{\rm d} \in \Omega^{\infty}$ that have a common prefix
of length $k$ (i.e, the first $k$ elements of $\overrightarrow{\rm c}$ and $\overrightarrow{\rm d}$ are the same), then we have
$$H_{\mathcal{M},\mathcal{A}, \overrightarrow{\rm c}}[k+1] = H_{\mathcal{M},\mathcal{A}, \overrightarrow{\rm d}}[k+1]$$
In this case, we say adversary $\mathcal{A}$ is an \emph{adaptive adversary}.

From the above definition, we can see an adaptive adversary cannot use
future coin flips to make current scheduling decisions.

\section{The Dynamic Task Alloction Problem}

% * \emph{define what is task? what is perform a task and insert a task?}

\subsection{Task}
Performing a task can be be any execution performed by a single
process in constant time and the tasks are assumed to satisfy the following three
properties \cite{georgiou2007all}:
\emph{Similarity}: The execution consume equal or comparable resources.
\emph{Independence}: One execution is independent of any of the others.
\emph{Idempotence}: Tasks could be performed more than one time.

For simplicity, in our thesis, \emph{tasks} are abstracted and defined as zero-valued shared registers
in an array, denoted as \texttt{tasks}$[ ]$.

The state of each register is either $0$ or $1$. When a register is in state $0$,
then we say a task is \emph{available}. When the
state of that register is flipped to $1$, we say that task is \emph{performed}.

Each array cell \texttt{tasks}$[i]$, where
integer $i \in \{0, 1, 2, ...\}$ is defined as a \emph{memory location} (or just \emph{location}) and denoted as $M$.
We say there is an available task \emph{associated} with location $M$ (or at $M$)
if the register at location $M$ is in state $0$.
Otherwise, we say there is no task at $M$ if the register is in state $1$.

Each task has a unique identifier from set
$\mathcal{L} = [m] = \{0, 1, ..., m\}$, where integer $m \in \{0, 1, 2, ...\}$.
A simple unique task identifier scheme is provided by \cite{alistarh2014dynamic}.

Informally, \emph{performing} a task is the execution that flipping the state of a register at some location from
$0$ to $1$. If the above value $m$ is fixed and the $m$ tasks are available initially,
and the execution ends when all tasks are performed, then we get the so-called \emph{static} version of task allocation problem,
i.e, write-all problem as follws: Given a zero-valued array of $m$ elements and $n$ processes,
write value $1$ into each array cell in the presence of adversity \cite{kanellakis1992efficient}.

In this thesis, we consider the \emph{dynamic} version of task allocation, i.e, Given $n$
asynchronous processes that cooperate to perform tasks, while tasks are \emph{inserted}
dynamically by the adversary during the execution, where \emph{inserting} a task is definied to be
the execution that flipping the state of a register from state $1$ back to $0$.
When the state of the register at location $M$
is flipped from $1$ to $0$, we also say a new task is \emph{inserted to locaton} $M$.

Since the state of the register at location $M$ can be flipped from $0$ to $1$ and from $1$ back to $0$
several times, thus we say one location $M$ can be associated with multiple tasks over time.

Now we specify the dynamic task alloction problem in terms of a type DTA, and the properties
that an implementation of type DTA must satisfy.

\subsection{The Type DTA}

The type DTA supports two types of operations, \texttt{DoTask()} and
\texttt{InsertTask(}$\ell$\texttt{)}, where $\ell$ is the task identifier.


\textbf{Operation DoTask()}.
The aim of operation \texttt{DoTask()} is to perform a task associated with location $M$ by calling
a special atomic operation \texttt{TryTask(}$M$\texttt{)} which is assumed to exist.

Task $\ell$ can be performed atomically by a process by calling \texttt{TryTask(}$M$\texttt{)} operation
with argument $M$ which is the memory location associated with task $\ell$. Out of several processes
calling \texttt{TryTask(}$M$\texttt{)}, only the first one receives notification $success$ and the identifier $\ell$ of
that task, while all the others only receive $failure$.

Every \texttt{DoTask()} operation may perform several \texttt{TryTask(}$M$\texttt{)}
operations. However, only one of them will succeed. Once one \texttt{TryTask(}$M$\texttt{)} succeeds, then
task identifier $\ell$ will be returned by \texttt{DoTask()}.
If there is no task could be performed, i.e, the state of each task is $1$,
then operation \texttt{DoTask()} returns $\perp$.


\textbf{Operation InsertTask($\ell$)}.
The goal of \texttt{InsertTask(}$\ell$\texttt{)} operation is to insert task $\ell$ to location $M$
also by calling a special atomic operation \texttt{PutTask(}$M,\ell$\texttt{)}.

Task $\ell$ can be
associated with memory location $M$ atomically by calling \texttt{PutTask(}$M,\ell$\texttt{)} operation.
\texttt{PutTask(}$M,\ell$\texttt{)} returns $success$ if task $\ell$ is associated with $M$,
and returns $failure$ if location $M$ was already associated with another task.

Several \texttt{PutTask(}$M,\ell$\texttt{)} operations may be executed but only one of them will succeed.
\texttt{PutTask(}$M,\ell$\texttt{)} fails if location $M$ has been associated with another task in state $1$.
Once one \texttt{TryTask(}$M$\texttt{)} succeeds, then task $\ell$ is available on location $M$ and $success$
notification is returned by \texttt{InsertTask(}$\ell$\texttt{)} operation.

Similarly, atomic operations \texttt{PutTask(}$M,\ell$\texttt{)} it assumed to be contained in \texttt{InsertTask(}$\ell$\texttt{)}
operation.

A task is performed if its identifier has been returned by a process after calling \texttt{DoTask()}.
A task $\ell$ is available at location $M$ if $success$ is returned by a process
after calling \texttt{InsertTask(}$\ell$\texttt{)}, but is not performed yet.

\textbf{Properties}.
An algorithm that accesses an instance of an object of type DTA must satisfy the following:

\emph{Validity}. If a \texttt{DoTask()} operation returns $\ell$, then before the
\texttt{DoTask()} operation, an \texttt{InsertTask(}$\ell$\texttt{)} was executed and returned $success$.

\emph{Uniqueness}. Each task is performed at most once, i.e, for each task $\ell$, at most one \texttt{DoTask()}
operation returns $\ell$.

In addition, the property that every inserted task is eventually done is also a desired progress property
of the implementation of type DTA.
