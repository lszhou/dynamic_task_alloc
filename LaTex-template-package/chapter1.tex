%--------------------CHAPTER 1-----------------------------
\chapter{Introduction}
In this chapter, we will introduce the task allocation problem as well as its dynamic version and survey the past work related to it.
\section{The Task Allocation Problem}
Task allocation problem, also called do-all problem, is already a foundational problem in distributing computing domain. During the last two decades, significant research was dedicated to studying the task allocation problem in various models of computation, including message passing, shared memory, etc. under specific assumption about the asynchrony and failures.

In this thesis, we consider the dynamic and shared memory version of the task allocation problem. The dynamic task allocation via asynchronous shared memory problem could be described as follows:

\emph{p processes must cooperatively perform a set of tasks in the presence of adversity. The tasks are injected by the adversary dynamically over time.}

\section{Related Work}
The task allocation via shared memory was firstly discussed by Kanellakis and Shavartsman[1]. In their paper about PRAM algorithm, they gave a robust parallel solution to the task allocation problem. The data structures they used are four full binary trees and tasks are abstracted as registers in an array. Each cell of the array is initialized with 0, and when it is turned to be 1 implies the abstracted task is performed successfully.

Up to now, there have been many research results focus on the topic. ..

Dan Alistarh and Michael, etc.

However, the above papers about the task allocation problem all focus on the static version, or called one-shot version, where the tasks are available at the beginning, and the execution is done when all these tasks are performed successfully. Dan Alistarh and Michael, etc. considered the dynamic version of task allocation problem, i.e. given p asynchronous processes that cooperate to perform tasks while the tasks are not available but inserted dynamically by the adversary during the execution. In that paper, they gave the first asynchronous shared memory algorithm for the dynamic task allocation, and proved that their algorithm is optimal within logarithmic factors. The main idea is to use a full binary full tree, called To-do tree which is inherited from their last paper, to guide the processes to insert the tasks at random empty memory locations and to pick newly inserted tasks to perform at random.


\section{Statement of Results}
In this thesis, we will introduce a randomized adaptive To-do tree algorithm which is similar to that presented by Dan Alistarh and Michael, etc. in their paper […]. The performance of our algorithm depends on the input sequence of tasks but not a fixed value. It has no constraint on the number of tasks presented in the to-do tree. Additionally, our algorithm implemented by the compare-and-swap registers directly which makes the complexity of our algorithm to be quadratic.

In chapter 2, we will formally define the dynamic task allocation problem again and describe out adaptive To-do tree algorithm.

In chapter 3, we will give a framework for the performance analysis of our algorithm.

In chapter 4, we summarize our results, relate them to past work and discuss the open questions that arise from our work.

%--------------------CHAPTER 2-----------------------------
\chapter{The Model and Problem Statement}
In this chapter, we will formally describe the system model we are working in and introduce the adaptive To-do tree algorithm, a randomized algorithm that could help us solve the dynamic asynchronous task allocation problem better.

\section{Standard Model}
The computation model we work in is the standard asynchronous shared memory mode with p processes 1, 2… p, where up to p-1 processes may fail by crashing. The processes communicate with each other via the shared registered, which they could perform read, write and compare-and-swap (CAS) operations. Each process has a unique identifier i which is from an unbounded namespace and has a local random number generator to perform coin flip operations.

The mode we discuss is under the control of a strong adaptive adversary. At any point of time, it can see the entire history (all coin flip operations and the returned values). Depending on such history, it decides which processes take the next step. i.e. At any point of time, the adversary knows exactly which step each process will be executing next.

\section{Problem Statement}
(问题的本质， 困难在哪) Dynamic asynchronous task allocation is the problem in which the p processes cooperatively execute tasks and the task are inserted into the data structure dynamically during the executing.

[What is task] For simplicity, the tasks are still abstracted and associated with shared registers. If a process changes the value of certain register from 0 to 1 successfully, it means the task associated with this register is performed; if a process changes the value of certain register from 1 to 0, it means the task is inserted successfully and associated with that register. Additionally, we assume each task has a unique identifier l>=0. One simple implementation of such task identifier uniqueness is to assign each task with an identifier in the form (id, count), where id the id of the process to which the task is assigned and count is the value of a local per-process counter, which is incremented on each newly inserted task.

[Complexity Metric] The complexity measure we use is the total number of operations (i.e. read, write, and CAS) that processes take during an execution.\\

\section{The Dynamic Adaptive To-Do Tree}
[some concepts and definition, e.g. tree, counts, surplus, space]
[Double Compare and Swap]
[Intro and detailed explanation of DoTask and InsertTask]


\begin{algorithm}
\caption{DoTask()}               %标题
\label{alg1}                         %标记算法，方便在其它地方引用
\begin{algorithmic}[1]
\WHILE {(true)}
   \STATE $v \leftarrow root$\\
   \IF {($v.surplus() \leq 0$)}
   \RETURN $\perp$
   \ENDIF \\
   \vspace{3mm}
   /* Descent */
   \WHILE {$v$ is not a leaf}
       \STATE $(x_L, y_L) \leftarrow v.left.read()$\\
       \STATE $(x_R, y_R) \leftarrow v.right.read()$\\
       \STATE $s_L \leftarrow min(x_L-y_L, 2^{height(v)})$ \\
       \STATE $s_R \leftarrow min(x_R-y_R, 2^{height(v)})$ \\
       \STATE $r \leftarrow random(0,1)$
       \IF {($(s_L+s_R)=0$)}
           \STATE Mark-up($v$)
       \ELSIF {($r<s_L/(s_L+s_R)$)}
           \STATE $v \leftarrow v.left$
       \ELSE
           \STATE $v \leftarrow v.rght$
       \ENDIF
   \ENDWHILE
   \vspace{3mm}\\
   /* $v$ is a leaf */
   \STATE $(x,y) \leftarrow v.read()$
   \STATE $(flag, l)$ $\leftarrow$ $v$.TryTask(task$[y+1]$)
   \STATE $v$.CAS$((x, y), (x, y+1))$    // Update Insertion Count
   \STATE $v \leftarrow v.parent$
   \STATE Mark-up($v$)
   \IF {$flag$ = success}
       \RETURN $l$
   \ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Insert(task $l$)}               %标题
\label{alg2}                         %标记算法，方便在其它地方引用
\begin{algorithmic}[1]
\WHILE {(true)}
   \STATE $v \leftarrow root$\\
   \vspace{3mm}
   /* Descent */
   \WHILE {($v$ is not a leaf)}
       \STATE $(x_L, y_L) \leftarrow v.left.read()$\\
       \STATE $(x_R, y_R) \leftarrow v.right.read()$\\
       \STATE $s_L \leftarrow min(x_L-y_L, 2^{height(v)})$ \\
       \STATE $s_R \leftarrow min(x_R-y_R, 2^{height(v)})$ \\
       \STATE $r \leftarrow random(0,1)$
       \IF {($(s_L+s_R) = 0$)}
           \STATE Mark-up($v$)
       \ELSIF {($r < s_L/(s_L + s_R)$)}
           \STATE $v \leftarrow v.left$
       \ELSE
           \STATE $v \leftarrow v.rght$
       \ENDIF
   \ENDWHILE
   \vspace{3mm}\\
   /* $v$ is a leaf */
   \STATE $(x,y) \leftarrow v.read()$
   \STATE $flag$ $\leftarrow$ $v$.PutTask(task$[x+1]$, $l$)
   \STATE $v$.CAS$((x,y), (x+1,y))$ // Update Insertion Count
   \STATE $v \leftarrow v.parent$
   \STATE Mark-up($v$)
   \IF {($flag$ = success)}
       \RETURN $success$
   \ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Mark-up($v$)}               %标题
\label{alg3}                         %标记算法，方便在其它地方引用
\begin{algorithmic}[1]
\IF{($v$ is not Null)}
    \FOR {($i=0; i<2; i++$)}
    \STATE $(x, y) \leftarrow v.read()$
    \STATE $(x_L, y_L) \leftarrow v.left.read()$
    \STATE $(x_R, y_R) \leftarrow v.right.read()$
    \STATE $v$.CAS$((x,y), (max(x, x_L + x_R), max(y, y_L + y_R))$\\
    \ENDFOR
\ENDIF
\end{algorithmic}
\end{algorithm}
%--------------------CHAPTER 3-----------------------------
\theoremstyle{definition}
\chapter{Analysis}
\section{Correctness}
The standard correctness condition for shared memory algorithm is linearizability which was introduced by M. Herlihy and J. M. Wing in 1990. The intuition of linearizability is the real-time behavior of method calls must be preserved, i.e. If one method call precedes another, then the earlier call must have taken effect before the later one. By contrast, if two method calls overlap, the we are free to order them in any convenient way since the order is ambiguous. Informally, a concurrent object is linearizable if each method call appears to take effect instantaneously at some moment between that its invocation and response. In the next section, we will first build the formal definition and terminology for linearizability, then prove our concurrent dynamic task allocation object is linearizable.

\subsection{Definitions and Terminology}
%An implementation is linearizable if for every execution, there is a total order of all completed operations and a subset of the uncompleted operations in the execution that satisfies the sequential specifications of the object and is consistent with the real-time order of these operations. The basic rule behind Linearizability is that every concurrent history is equivalent to some sequential history. If one method call proceeds another, then the other one mush have taken effect before the later one call. By contrast, if two method calls overlap, then we are free to order them in any convenient way.

\begin{definition}{\textbf{History}.}
A history  (or execution) $H$, obtained by processes executing shared memory operations on concurrent objects, is a finite sequence of method invocation and response events.
\end{definition}

A history $H$ could be related to many concurrent objects. When we consider the concurrency behavior of a specific object, we should focus on the subhistory of that object which is defined as follows:

\begin{definition}{\textbf{Subhistory}.}
The subhistory $H|obj$ of history $H$ is the subsequence of all invocation and response events in $H$ whose object names are $obj$.
\end{definition}

If a history $H$ we consider is related to only one object $obj$, then the subhistory $H_{obj}$ is right the history $H$. Thus, in the following discussion, when we discuss the concurrency behavior of a specific objet $obj$, the history $H$ and the subhistory $H_{obj}$  have no difference.

We use $inv_H(op)$ to denote the position of the invocation of operation $op$ in history $H$. Similarly we use $rep_H(op)$ to denote the position of the corresponding response of $op$ in $H$. Actually, for each operation $op$, it is not necessary to have a matching response. In this case, we call the operation $op$ is pending and denote it as $rsp_H(op) = \infty$. Otherwise, we call the operation is complete in $H$.

\begin{definition}{\textbf{Complete/Imcomplete History}.}
A history is complete if all its operation are complete. A completion of an incomplete history $H$ is an extension $H'$ of $H$, such that $H'$ contains exactly the same operations as $H$ but the responses are added for the pending operations in $H$.
\end{definition}

Let $H$ be a complete history. With each operation $op$ in $H$ we could associate a time interval $I_H(op) = [inv(op), rsp(op)]$. Similarity, for an uncomplete history, we denote the interval with respect to the pending operation $op$ by $I_H(op) = [inv(op), \infty]$.

\begin{definition}{\textbf{Sequential History}.}
A history is sequential if the first event in the history is an invocation, and each invocation, except possibly the last one, is immediately followed by a corresponding response.
\end{definition}

If a history is sequential then we denote it as $S$ in order to distinguish it with non-sequential or uncertain history $H$. It is obvious a sequential history is a total order over all operations.

\begin{definition}{\textbf{Sequential Specification}.}
The sequential specification $S_{obj}$ for a concurrent object $obj$ is a set of sequential histories for $obj$.
\end{definition}

A sequential history $S$ satisfies the sequential specification of object $obj$ means $S$ is contained in the above sequential history set $S_{obj}$, i.e, $S \in S_{obj}$.

\begin{definition}{\textbf{Valid Sequential History}.}
A sequential history $S$ of object $obj$ is valid (sometimes also called legal) if it satisfies the sequential specification of $obj$
\end{definition}

With the above definitions, we could now define linearization as follows:

\begin{definition}{\textbf{Linearization}.}
A history $H$ linearizes to a sequential history $S$, if and only if $S$ satisfies the following conditions: (1) $S$ and $H$ have the same operations, (2) sequential history $S$ is valid, and (3) there is a mapping from each time interval $I_H(op)$ to a time point $t_H(op)$, $t_H(op) \in I_H(op)$, such that the sequential history $S$ could be obtained by sorting the operations in $H$ by their $t_H(op)$.
\end{definition}

In another word, a history $H$ is linearizable if and only if we could find a such a sequential history $S$ which satisfies the above three conditions.

\begin{definition}{\textbf{Linearization Point}.}
For each operation $op$, we call the time point $t_H(op)$ described in the above definition the linearization point of operation $op$.
\end{definition}

\begin{definition}{\textbf{Linearizable Object}.}
A concurrent object is linearizable if every history $H$ of that object is linearizable.
\end{definition}

In the following subsection, we will prove our concurrent dynamic task allocation object shown in Figure 1 is linearizable. 

%=========================================================================================================================
%===================================================== I am here! ==========================================================
%===========================================================================================================================

\subsection{Analysis and Proofs}

Through the definitions in subsection 3.1.1, we know that one way to show a concurrent object $obj$ is linearizable is to prove its arbitrary history $H$ is linearizable. Specifically, to obtain the linearizability of $obj$, we need to identify for each DoTask or InsertTask operation $op$ (i.e, interval $I_H(op)$) in $H$ a linearization point, and prove that the sequential history obtained by sorting these operations according to their linearization points satisfies the sequential specification of $obj$.

We notice that each complete DoTask or InsertTask operation can be associated with a unique index of the task array corresponding to the task it removed or inserted. The insertion count and removal count are monotonically increasing. Thus, we could associate the node counts with operations which have been propagated to that node. Next, we are going to define ``an operation is counted at a node" to formalize the operation propagation. 

The DoTask operation is counted at a leaf when some process updates the removal count with the index of the task array slot its successful PutTask operation deals with. The definition for InsertTask is symmetric.

Now we define a DoTask operation is counted at an arbitrary inner node $v$ and counting a InsertTask is symmetric as well. 

Recall that we use linearizable CAS operation to update the inner node counts, we could therefore linearize all such operations that update the removal count of $v$ with the same value $y$. For all these operations, only the first one in the linearization order counts the corresponding DoTask operation. In another word, a DoTask operation is counted at an inner node $v$ as soon as the updating operation that counts the DoTask is linearized. 
Please note that, the update counting the DoTask at node $v$ is not necessary performed by the DoTask operation itself, i.e, suppose process $p$ is executes a DoTask operation and has successfully performed task $l$ at certain leaf. Then the operation that counts this DoTask at node $v$ could be a different process $q$ if $q$ updates the removal count first in the linearization order. 

Given the above concepts and properties, we could prove the following results:

\begin{lem}
All complete DoTask or InsertTask operations $op$ will finally be counted at each node.
\end{lem}
\begin{proof}
[Sketch] This is ensured by the repetition of double compare and swap operation during the execution of mark-up($v$).
\end{proof}

\begin{lem}
The point $t_H(op)$ when $op$ is counted at the root must be between $inv_H(op)$ and $rsp_H(op)$.
\end{lem}
\begin{proof}
The fact $t_H(op) > inv_H(op)$ is needless to prove. Without loss of generality, suppose process $p$ executs $op$ and $op$ is a DoTask operation which has performed some task successfully at the leaf. When it walks back to root and executes Mark-up(root), there will be two cases:

Case 1:  It increments the removal count successfully via CAS (line 6, method 3) at point $\tau$. If it is the first one in the linearization order of all operations updating the removal count with the same value, then $t_H(op) = \tau$, therefore $t_H(op) < rsp_H(op)$. Otherwise, we could let $t_H(op) = \tau'$, where $\tau'$ is the first one in the linearization order updated the removal count of root. Obviously, $\tau' < \tau$, therefore $t_H(op) < rsp_H(op)$.

Case 2: It fails to increment the removal count. The compare and swap operation of $p$ fails if and only if the value of the insertion and/or removal counts are/is updated at a point before $\tau$, suppose it is $\tau'$. Here are two subcases.

Subcase 2.1: If the removal count is incremented at $\tau'$, it means the DoTask operation has already been counted by another process. Thus, $t_H(op) \leq \tau' < rsp_H(op)$.

Subcase 2.2: If it is not the removal count but the insertion count that is updated at $\tau'$. We notice that the CAS operation (line 6, method 3) will be repeated by $p$, during the second iteration, if the CAS of $p$ succeed at $\tau''$, then we could deduce that $t_H(op) \leq \tau''$, therefore  $t_H(op) < rsp_H(op)$. If it fails again at point $\tau''$ , then there must be another process updated the counts of root again. This time, the counts of the children will be noticed and the DoTask operation will propagate to the root. We could deduce $t_H(op) < \tau''$. Thus,  $t_H(op) < rsp_H(op)$ as well.
\end{proof}

Under the above definitions and properties, we claim the point $t_H(op)$ when $op$ is counted at the root is the linearization point of operation $op$.

\begin{lem}
The dynamic task allocation object in the above figure is linearizable.
\end{lem}
\begin{proof}
Consider an arbitrary history $H$ containing DoTask and InsertTask operations. We should prove for any execution of our algorithm, the total order given by the linearization point is verifies the uniqueness and validity. If $H$ is not complete, then we let all processes that have not finished their operations continue to take steps in an arbitrary order until all operations are completed. Every operation will finally be done is ensured by our computation model and the randomness of our algorithm. This way we obtain a completion $H'$ of $H$ and it suffices to prove $H'$ is linearizable. Thus, to prove this lemma, we should prove the total order obtained by sorting the operations by their $t_H(op)$ values is valid.

\texttt{The uniqueness is obvious. When multiple processes are calling TryTask($l$) at the memory location, only one of them will receive success and the index $l$ of the task, all the other competitor processes will get failure. Task $l$ is performed successfully as long as the value of corresponding memory location is turned to 1. The following process will never repeatedly turn it be to 0 and turn 0 to 1 which is guaranteed by the our semantics of task insertion and removing.}

Now we prove the validity, i.e. each task that is performed successfully must have been inserted before. We should prove the insertion operation of a task is always counted at the root before the removal operation. To prove this result holds for the root, we now prove it by induction from the leaf.

At the leaf, this holds because if and only if the insertion count of newly inserted task $l$ has been incremented (line 19, method 2) then the following removal operation could read that (line 20, method 1) to know the available task $l$ at the leaf and then try to perform it. In another word, suppose the task $l$ is inserted but the insertion count is not incremented (i.e. insertion has not been counted yet), then the following removal operation has no way to know the available task $l$, perform it and increment the removal count. Thus, the removal will not be counted.

For an arbitrary inner node $v$, we suppose, by the induction step and lemma 1, the result holds for children $v.left$ and $v.right$. We could notice that any process updates the insertion count and removal count as an atomic operation (line 6, method 3). If some remove operation has been counted at node $v$, then the corresponding insert operation for that task must have been counted at $v$ simultaneously by the double compare-and-swap operation. Apply this to the root, then validity condition holds.
\end{proof}
\section{Performance}
\subsection{DoTask Analysis}
\subsection{InsertTask Analysis}
\section{Competitive Analysis}
%--------------------CHAPTER 1-----------------------------
\chapter{Conclusions}
\section{Contributions}
\section{Future Work}

