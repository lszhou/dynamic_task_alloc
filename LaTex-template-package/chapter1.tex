%--------------------CHAPTER 1-----------------------------
\chapter{Introduction}
In this chapter, we will introduce the task allocation problem as well as its dynamic version and survey the past work related to it.
\section{The Dynamic Task Allocation Problem}
Task allocation problem, also called do-all problem, is already a foundational topic in distributing computing. During the last two decades, significant research was dedicated to studying the task allocation problem in various models of computation, including message passing, shared memory, etc. under specific assumption about the asynchrony and failures.

In this thesis, we consider the dynamic version of the task allocation problem via randomized asynchronous shared memory. The dynamic version could be described as follows:

\emph{p processes cooperatively perform a set of tasks in the presence of adversity and the tasks are injected dynamically by the adversary during the execution.}

\section{Related Work}
%The task allocation via shared memory was firstly discussed by Kanellakis and Shavartsman[1]. In their paper about PRAM algorithm, they gave a robust parallel solution to the task allocation problem. The data structures they used are four full binary trees and tasks are abstracted as registers in an array. Each cell of the array is initialized with 0, and when it is turned to be 1 implies the abstracted task is performed successfully.
%
%Up to now, there have been many research results focus on the topic. ..
%
%Dan Alistarh and Michael, etc.
%
%However, the above papers about the task allocation problem all focus on the static version, or called one-shot version, where the tasks are available at the beginning, and the execution is done when all these tasks are performed successfully. Dan Alistarh and Michael, etc. considered the dynamic version of task allocation problem, i.e. given p asynchronous processes that cooperate to perform tasks while the tasks are not available but inserted dynamically by the adversary during the execution. In that paper, they gave the first asynchronous shared memory algorithm for the dynamic task allocation, and proved that their algorithm is optimal within logarithmic factors. The main idea is to use a full binary full tree, called To-do tree which is inherited from their last paper, to guide the processes to insert the tasks at random empty memory locations and to pick newly inserted tasks to perform at random.


\section{Statement of Results}
%To solve the dynamic task allocation problem, we implement the dynamic task allocation type by a concurrent data structure called adaptive To-do tree. The dynamic inserted tasks are stored in the shared-memory bit array at the leaves. Multiple processes could get access to the bit array to perform and insert a task via the randomized DoTask and InsertTask operations supported by the To-do tree.
%
%In this thesis, we will introduce a randomized adaptive To-do tree algorithm which is similar to that presented by Dan Alistarh and Michael, etc. in their paper […]. The performance of our algorithm depends on the input sequence of tasks but not a fixed value. It has no constraint on the number of tasks presented in the to-do tree. Additionally, our algorithm implemented by the compare-and-swap registers directly which makes the complexity of our algorithm to be quadratic.
%
%In chapter 2, we will formally define the dynamic task allocation problem again and describe out adaptive To-do tree algorithm.
%
%In chapter 3, we will give a framework for the performance analysis of our algorithm.
%
%In chapter 4, we summarize our results, relate them to past work and discuss the open questions that arise from our work.

%--------------------CHAPTER 2-----------------------------
\chapter{The Computation Model and Algorithms}
In this chapter, we will formally describe the system model we are working in and introduce our dynamic task allocation project.

\section{Computation Model}
The computation model we consider is the standard asynchronous shared memory mode with $p$ processes $1, 2, ..., p$, where up to $p-1$ processes may fail by crashing. The processes communicate with each other via the shared double-compare-swap (DCAS) registers. Each process executes its program by taking steps which are defined to be the executions of all local computations followed by an operation on a shared register.

A DCAS register $v$ is a shared memory object stores a pair of two values from some set and support two atomic synchronization operations $v.read()$ and $v.DCAS((x,y), (a, b))$. Operation $v.read()$ returns the two values of the register and leaves their content unchanged. Operation $v.DCAS((x,y), (a, b))$ is an extension of the compare-and-swap (CAS) operation on two memory locations. A $v.DCAS((x,y), (a, b))$ takes two memory locations and writes new values into them only if they match pre-supplied ``expected" values $(x, y)$, i.e, if current values of $v$ are exactly $(x, y)$, then the operation $v.DCAS((x,y), (a, b))$ succeeds, and the values of $v$ are changed to be $(a, b)$ and $true$ is returned. Otherwise, the operation fails, the current values of $v$ remain unchanged and $false$ is returned.

Each process has a unique identifier $i$ which is from an unbounded namespace has a local random number generator to perform coin flip operations. In the following pseudocode, we use $random(0, 1)$ to return a value distributed uniformly at random from the set $(o, 1)$.

In addition, the mode we discuss is under the control of a strong adaptive adversary. At any point of time, it can see the entire history (all coin flip operations and the returned values). Depending on such history, it decides which processes take the next step. i.e. At any point of time, the adversary knows exactly which step each process will be executing next.

\section{Problem Statement}
.......
%Dynamic asynchronous task allocation is the problem in which the p processes cooperatively execute tasks and the task are inserted into the data structure dynamically during the executing.
%
%For simplicity, the tasks are still abstracted and associated with shared registers. If a process changes the value of certain register from 0 to 1 successfully, it means the task associated with this register is performed; if a process changes the value of certain register from 1 to 0, it means the task is inserted successfully and associated with that register. Additionally, we assume each task has a unique identifier l>=0. One simple implementation of such task identifier uniqueness is to assign each task with an identifier in the form (id, count), where id the id of the process to which the task is assigned and count is the value of a local per-process counter, which is incremented on each newly inserted task.
%
%Define an InsertTask operation is successful if it has inserted a task into the task array. Similarly, a DoTask operation is successful if it has performed a task at a leaf. A complete InsertTask or DoTask operation must be a successful operation, but not vice versa.

\subsection*{Complexity Metric}
The complexity measure we use is the total number of shared memory operations that processes take during an execution. Here the operations include compare and swap, read and write.

\subsection*{Sequential Specification}
A shared object is an instance of a type, which supports some set of operations and satisfies the sequential specification. Type $dynamic\_task\_alloc$ supports two operation $DoTask()$ and $InsertTask(task$ $l)$. The $DoTask$ operation performs a new task and returns the index of that task. If there is no available task currently, then it returns $\perp$. The $InsertTask(task$ $l)$ operation insert the task $l$ to be performed and return $success$ if $l$ is inserted successfully.

The sequential specification of type $dynamic\_task\_alloc$ is presented as follows:\\
\noindent $\bullet$ Validity: If a $DoTask($ $)$ returns $l$, then before that, there was a $InsertTask(task$ $l)$.\\
\noindent $\bullet$ Uniqueness: For each task $l$, only one $InsertTask(task$ $l)$ returns $success$ and only one $DoTask($ $)$ returns $l$.\\
\noindent $\bullet$ Fairness: Every inserted task $l$ will finally be performed.




\section{The Adaptive To-Do Tree}
%[some concepts and definition, e.g. tree, counts, surplus, space]
%[Double Compare and Swap]
%[Intro and detailed explanation of DoTask and InsertTask]

............
\begin{algorithm}
\caption{DoTask()}               %标题
\label{alg1}                         %标记算法，方便在其它地方引用
\begin{algorithmic}[1]
\WHILE {(true)}
   \STATE $v \leftarrow root$\\
   \IF {($v.surplus() \leq 0$)}
   \RETURN $\perp$
   \ENDIF \\
   \vspace{3mm}
   /* Descent */
   \WHILE {$v$ is not a leaf}
       \STATE $(x_L, y_L) \leftarrow v.left.read()$\\
       \STATE $(x_R, y_R) \leftarrow v.right.read()$\\
       \STATE $s_L \leftarrow min(x_L-y_L, 2^{height(v)})$ \\
       \STATE $s_R \leftarrow min(x_R-y_R, 2^{height(v)})$ \\
       \STATE $r \leftarrow random(0,1)$
       \IF {($(s_L+s_R)=0$)}
           \STATE Mark-up($v$)
       \ELSIF {($r<s_L/(s_L+s_R)$)}
           \STATE $v \leftarrow v.left$
       \ELSE
           \STATE $v \leftarrow v.rght$
       \ENDIF
   \ENDWHILE
   \vspace{3mm}\\
   /* $v$ is a leaf */
   \STATE $(x,y) \leftarrow v.read()$
   \STATE $(flag, l)$ $\leftarrow$ $v$.TryTask(task$[y+1]$)
   \STATE $v$.DCAS$((x, y), (x, y+1))$    // Update Insertion Count
   \STATE $v \leftarrow v.parent$
   \STATE Mark-up($v$)
   \IF {$flag$ = success}
       \RETURN $l$
   \ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Insert(task $l$)}               %标题
\label{alg2}                         %标记算法，方便在其它地方引用
\begin{algorithmic}[1]
\WHILE {(true)}
   \STATE $v \leftarrow root$\\
   \vspace{3mm}
   /* Descent */
   \WHILE {($v$ is not a leaf)}
       \STATE $(x_L, y_L) \leftarrow v.left.read()$\\
       \STATE $(x_R, y_R) \leftarrow v.right.read()$\\
       \STATE $s_L \leftarrow min(x_L-y_L, 2^{height(v)})$ \\
       \STATE $s_R \leftarrow min(x_R-y_R, 2^{height(v)})$ \\
       \STATE $r \leftarrow random(0,1)$
       \IF {($(s_L+s_R) = 0$)}
           \STATE Mark-up($v$)
       \ELSIF {($r < s_L/(s_L + s_R)$)}
           \STATE $v \leftarrow v.left$
       \ELSE
           \STATE $v \leftarrow v.rght$
       \ENDIF
   \ENDWHILE
   \vspace{3mm}\\
   /* $v$ is a leaf */
   \STATE $(x,y) \leftarrow v.read()$
   \STATE $flag$ $\leftarrow$ $v$.PutTask(task$[x+1]$, $l$)
   \STATE $v$.DCAS$((x,y), (x+1,y))$ // Update Insertion Count
   \STATE $v \leftarrow v.parent$
   \STATE Mark-up($v$)
   \IF {($flag$ = success)}
       \RETURN $success$
   \ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Mark-up($v$)}               %标题
\label{alg3}                         %标记算法，方便在其它地方引用
\begin{algorithmic}[1]
\IF{($v$ is not Null)}
    \FOR {($i=0; i<2; i++$)}
    \STATE $(x, y) \leftarrow v.read()$
    \STATE $(x_L, y_L) \leftarrow v.left.read()$
    \STATE $(x_R, y_R) \leftarrow v.right.read()$
    \STATE $v$.DCAS$((x,y), (max(x, x_L + x_R), max(y, y_L + y_R))$\\
    \ENDFOR
\ENDIF
\end{algorithmic}
\end{algorithm}
%--------------------CHAPTER 3-----------------------------
\theoremstyle{definition}
\chapter{Analysis}
\section{Correctness}
The standard correctness condition for shared memory algorithm is linearizability which was introduced by M. Herlihy and J. M. Wing in 1990. The intuition of linearizability is the real-time behavior of method calls must be preserved, i.e. If one method call precedes another, then the earlier call must have taken effect before the later one. By contrast, if two method calls overlap, the we are free to order them in any convenient way since the order is ambiguous. Informally, a concurrent object is linearizable if each method call appears to take effect instantaneously at some moment between that its invocation and response. In the next section, we will first build the formal definition and terminology for linearizability, then prove our concurrent dynamic task allocation object is linearizable.

\subsection{Definitions and Terminology}
%An implementation is linearizable if for every execution, there is a total order of all completed operations and a subset of the uncompleted operations in the execution that satisfies the sequential specifications of the object and is consistent with the real-time order of these operations. The basic rule behind Linearizability is that every concurrent history is equivalent to some sequential history. If one method call proceeds another, then the other one mush have taken effect before the later one call. By contrast, if two method calls overlap, then we are free to order them in any convenient way.

\begin{definition}{\textbf{History}.}
A history  (or execution) $H$, obtained by processes executing shared memory operations on concurrent objects, is a finite sequence of method invocation and response events.
\end{definition}

A history $H$ may be related to several concurrent objects. When we consider the concurrency behavior of a specific object, we should focus on the subhistory of that object which is defined as follows:

\begin{definition}{\textbf{Subhistory}.}
The subhistory $H|obj$ of history $H$ is the subsequence of all invocation and response events in $H$ whose object names are all $obj$.
\end{definition}

If a history $H$ we consider is related to only one object $obj$, then the subhistory $H_{obj}$ is right the history $H$. Thus, in the following discussion, when we discuss the concurrency behavior of a specific objet $obj$, the history $H$ and the subhistory $H_{obj}$  have no difference.

We use $inv_H(op)$ to denote the position of the invocation of operation $op$ in history $H$. Similarly we use $rep_H(op)$ to denote the position of the corresponding response of $op$ in $H$. Actually, for each operation $op$, it is not necessary to have a matching response. In this case, we call the operation $op$ is pending and denote it as $rsp_H(op) = \infty$. Otherwise, we call the operation is complete in $H$.

\begin{definition}{\textbf{Complete/Imcomplete History}.}
A history is complete if all its operations are complete. A completion of an incomplete history $H$ is an extension $H'$ of $H$, such that $H'$ contains exactly the same operations as $H$ but the responses are added for the pending operations in $H$.
\end{definition}

Let $H$ be a complete history. With each operation $op$ in $H$ we could associate a time interval $I_H(op) = [inv(op), rsp(op)]$. Similarly, for an uncomplete history, we denote the interval with respect to the pending operation $op$ by $I_H(op) = [inv(op), \infty]$.

\begin{definition}{\textbf{Sequential History}.}
A history is sequential if the first event in the history is an invocation, and each invocation, except possibly the last one, is immediately followed by a corresponding response.
\end{definition}

It is obvious that a sequential history is a total order over all operations. In order to distinguish with the history $H$ we defined in definition 1, we use $S$ to denote a sequential history.

\begin{definition}{\textbf{Sequential Specification}.}
The sequential specification $S_{obj}$ for a concurrent object $obj$ is the set of all possible sequential histories for $obj$.
\end{definition}

We say a sequential history $S$ satisfies the sequential specification of object $obj$ means $S$ is contained in the above sequential history set $S_{obj}$, i.e, $S \in S_{obj}$.

\begin{definition}{\textbf{Valid Sequential History}.}
A sequential history $S$ of object $obj$ is valid (sometimes also called legal) if it satisfies the sequential specification of $obj$.
\end{definition}

With the above definitions, we could now define linearization as follows:

\begin{definition}{\textbf{Linearization}.}
A history $H$ linearizes to a sequential history $S$, if and only if $S$ satisfies the following conditions: (1) $S$ and $H$ have the same operations, (2) sequential history $S$ is valid, and (3) there is a mapping from each time interval $I_H(op)$ to a time point $t_H(op)$, $t_H(op) \in I_H(op)$, such that the sequential history $S$ could be obtained by sorting the operations in $H$ by their $t_H(op)$.
\end{definition}

In another word, a history $H$ is linearizable if and only if we could find a such a sequential history $S$ which satisfies the above three conditions.

\begin{definition}{\textbf{Linearization Point}.}
For each operation $op$ in history $H$, we call the time point $t_H(op)$ described in the above definition the linearization point of $op$.
\end{definition}

\begin{definition}{\textbf{Linearizable Object}.}
A concurrent object is linearizable if every history $H$ of that object is linearizable.
\end{definition}

In the following subsection, we will prove our concurrent dynamic task allocation object shown in Figure 1 is linearizable.



\subsection{Analysis and Proofs}

Through the definitions in subsection 3.1.1, one way to show a concurrent object $obj$ is linearizable is to prove its arbitrary history $H$ is linearizable. Specifically, to obtain the linearizability of $obj$, we need to identify for each DoTask or InsertTask operation $op$ (i.e, interval $I_H(op)$) in $H$ a linearization point, and prove that the sequential history $S$ obtained by sorting these operations according to their linearization points satisfies the sequential specification of $obj$.

We notice that each complete DoTask or InsertTask operation can be associated with a unique task array slot with respect to the task it removed or inserted. Additionally, the removal count and insertion count are both monotonically increasing. Thus, we could associate the node counts with operations which have been propagated to that node. Next, we are going to define ``an operation is counted at a node" recursively to formalize the operation propagation.

A DoTask operation is counted at a leaf when the removal count at the leaf is updated with the index of the task array slot where the successfully performed task is located. The definition for InsertTask operation is symmetric.

Now we define a DoTask operation is counted at an arbitrary inner node $v$ and counting a InsertTask is symmetric as well.

Recall that the removal count of $v$ is always updated though CAS operation (line 6, method 3). Notice that there could be more than one operations updating the count with the same value. We could therefore linearize all these operations that update the removal count of $v$ with the same value $y$. For all these operations, only the first one in the linearization order counts the corresponding DoTask operation. In another word, a DoTask operation is counted at an inner node $v$ as soon as the updating operation that counts the DoTask is linearized. Based this definition, no operation will be counted twice at a node.

Please note that, the update counting the DoTask at node $v$ is not necessary performed by the DoTask operation itself, i.e, suppose process $p$ is executes a DoTask operation and has successfully performed task $l$ at certain leaf. Then the operation that counts this DoTask at node $v$ could be a different process $q$ if $q$ updates the removal count first in the linearization order.

Given the above concepts and properties, we could prove the following result:

\begin{lem}
Let $v$ be a tree node, \\
(1) If $(x, y)$ is the result by calling $v.read( )$, then there exists a set of $x$ successful InsertTask operations and a set of $y$ successful DoTask operation that have been counted at node $v$ by the end of the execution of $v.read( )$.\\
\noindent (2) If there are $x$ DoTask operations that have been counted at $v$ before the execution of $v.read( )$, then the removal count value returned by $v.read( )$ is not less than $x$.
\end{lem}
\begin{proof}
%[Sketch] This is ensured by the repetition of double compare and swap operation during the execution of mark-up($v$).
\end{proof}



\begin{lem}
Consider a history $H$ and an arbitrary operation $op$ in $H$, let $t_H(op)$ be the point when the operation $op$ is counted at the root, then $t_H(op)$ is between $inv_H(op)$ and $rsp_H(op)$.
\end{lem}
\begin{proof}
The fact $t_H(op) > inv_H(op)$ is needless to prove. Without loss of generality, suppose process $p$ executs $op$ and $op$ is a DoTask operation which has performed some task successfully at the leaf. When it walks back to the $root$ and executes Mark-up($root$), there will be two cases:

Case 1:  It increments the removal count of $root$ successfully via DCAS (line 6, method 3) at point $\tau$. If it is the first one in the linearization order of all updates of the removal count with the same value, then $t_H(op) = \tau$, therefore $t_H(op) < rsp_H(op)$. Otherwise, we could let $t_H(op) = \tau'$, where $\tau'$ is the first one in the linearization order updated the removal count of root. Because only if the task has been performed then the removal count of root could be updated. Thus, $\tau' < \tau$, therefore $t_H(op) < rsp_H(op)$.

Case 2: It fails to increment the removal count. The compare and swap operation of $p$ fails if and only if the value of the insertion and/or removal counts are/is updated at a point before $\tau$, suppose it is $\tau'$. Here are two subcases.

Subcase 2.1: If the removal count is incremented at $\tau'$, it means the DoTask operation has already been counted by another process. Thus, $t_H(op) \leq \tau' < rsp_H(op)$.

Subcase 2.2: If it is not the removal count but the insertion count that is updated at $\tau'$. We notice that the CAS operation (line 6, method 3) will be repeated by $p$, during the second iteration, if the CAS of $p$ succeed at $\tau''$, then we could deduce that $t_H(op) \leq \tau''$, therefore  $t_H(op) < rsp_H(op)$. If it fails again at point $\tau''$ , then there must be another process updated the counts of root again. This time, the counts of the children will be noticed and the DoTask operation will propagate to the root. We could deduce $t_H(op) < \tau''$. Thus,  $t_H(op) < rsp_H(op)$ as well.
\end{proof}

Under the above definitions and properties, we claim the point $t_H(op)$ when $op$ is counted at the root is the linearization point of operation $op$.

\begin{lem}
The dynamic task allocation object in the above figure is linearizable.
\end{lem}
\begin{proof}
Consider an arbitrary history $H$ containing DoTask and InsertTask operations. We should prove for any execution of our algorithm, the total order given by the linearization point is verifies the uniqueness and validity. If $H$ is not complete, then we let all processes that have not finished their operations continue to take steps in an arbitrary order until all operations are completed. Every operation will finally be done is ensured by our computation model and the randomness of our algorithm. This way we obtain a completion $H'$ of $H$ and it suffices to prove $H'$ is linearizable. Thus, to prove this lemma, we should prove the total order obtained by sorting the operations by their $t_H(op)$ values is valid.

\texttt{The uniqueness is obvious. When multiple processes are calling TryTask($l$) at the memory location, only one of them will receive success and the index $l$ of the task, all the other competitor processes will get failure. Task $l$ is performed successfully as long as the value of corresponding memory location is turned to 1. The following process will never repeatedly turn it be to 0 and turn 0 to 1 which is guaranteed by the our semantics of task insertion and removing.}

Now we prove the validity, i.e. each task that is performed successfully must have been inserted before. We should prove the insertion operation of a task is always counted at the root before the removal operation. To prove this result holds for the root, we now prove it by induction from the leaf.

At the leaf, this holds because if and only if the insertion count of newly inserted task $l$ has been incremented (line 19, method 2) then the following removal operation could read that (line 20, method 1) to know the available task $l$ at the leaf and then try to perform it. In another word, suppose the task $l$ is inserted but the insertion count is not incremented (i.e. insertion has not been counted yet), then the following removal operation has no way to know the available task $l$, perform it and increment the removal count. Thus, the removal will not be counted.

For an arbitrary inner node $v$, we suppose, by the induction step and lemma 1, the result holds for children $v.left$ and $v.right$. We could notice that any process updates the insertion count and removal count as an atomic operation (line 6, method 3). If some remove operation has been counted at node $v$, then the corresponding insert operation for that task must have been counted at $v$ simultaneously by the double compare-and-swap operation. Apply this to the root, then validity condition holds.
\end{proof}
\section{Performance}
\subsection{DoTask Analysis}
\subsection{InsertTask Analysis}
\section{Competitive Analysis}
%--------------------CHAPTER 1-----------------------------
\chapter{Conclusions}
\section{Contributions}
\section{Future Work}

