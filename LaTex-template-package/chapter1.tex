%--------------------CHAPTER 1-----------------------------
\chapter{Introduction}
In this chapter, we will introduce the task allocation problem as well as its dynamic version and survey the past work related to it.

Task allocation problem, also called do-all problem, is already a foundational topic in distributing computing. During the last two decades, significant research was dedicated to studying the task allocation problem in various models of computation, including message passing, shared memory, etc. under specific assumption about the asynchrony and failures.

In this thesis, we consider the dynamic version of the task allocation problem via randomized asynchronous shared memory. The dynamic version could be described as follows:

\emph{p processes cooperatively perform a set of tasks in the presence of adversity and the tasks are injected dynamically by the adversary during the execution.}

\section{Related Work}
%The task allocation via shared memory was firstly discussed by Kanellakis and Shavartsman[1]. In their paper about PRAM algorithm, they gave a robust parallel solution to the task allocation problem. The data structures they used are four full binary trees and tasks are abstracted as registers in an array. Each cell of the array is initialized with 0, and when it is turned to be 1 implies the abstracted task is performed successfully.
%
%Up to now, there have been many research results focus on the topic. ..
%
%Dan Alistarh and Michael, etc.
%
%However, the above papers about the task allocation problem all focus on the static version, or called one-shot version, where the tasks are available at the beginning, and the execution is done when all these tasks are performed successfully. Dan Alistarh and Michael, etc. considered the dynamic version of task allocation problem, i.e. given p asynchronous processes that cooperate to perform tasks while the tasks are not available but inserted dynamically by the adversary during the execution. In that paper, they gave the first asynchronous shared memory algorithm for the dynamic task allocation, and proved that their algorithm is optimal within logarithmic factors. The main idea is to use a full binary full tree, called To-do tree which is inherited from their last paper, to guide the processes to insert the tasks at random empty memory locations and to pick newly inserted tasks to perform at random.


\section{Statement of Results}
%To solve the dynamic task allocation problem, we implement the dynamic task allocation type by a concurrent data structure called adaptive To-do tree. The dynamic inserted tasks are stored in the shared-memory bit array at the leaves. Multiple processes could get access to the bit array to perform and insert a task via the randomized DoTask and InsertTask operations supported by the To-do tree.
%
%In this thesis, we will introduce a randomized adaptive To-do tree algorithm which is similar to that presented by Dan Alistarh and Michael, etc. in their paper […]. The performance of our algorithm depends on the input sequence of tasks but not a fixed value. It has no constraint on the number of tasks presented in the to-do tree. Additionally, our algorithm implemented by the compare-and-swap registers directly which makes the complexity of our algorithm to be quadratic.
%
%In chapter 2, we will formally define the dynamic task allocation problem again and describe out adaptive To-do tree algorithm.
%
%In chapter 3, we will give a framework for the performance analysis of our algorithm.
%
%In chapter 4, we summarize our results, relate them to past work and discuss the open questions that arise from our work.

%--------------------CHAPTER 2-----------------------------
\chapter{The Computational Model and Problem Statement}
In this chapter, we will formally describe the system model we are working in and introduce our dynamic task allocation object.

\section{Computational Model}
The computational model we consider is the standard asynchronous shared memory model with $n$ processes.  Every process executes its program by
taking steps and up to $n-1$ processes may fail by crashing. A step is defined to be the execution of all local computations
followed by an operation on a shared object. In this thesis, we consider the system that supports atomic compare-and-swap (CAS) object.

A CAS object $v$ is a shared memory object stores a value from some set and supports two atomic synchronization operations $v.read()$ and $v.CAS((x,y), (a, b))$. Operation $v.read()$ returns the current value of the object and leaves the value unchanged. A $v.CAS((x,y), (a, b))$ takes the memory locations and writes new value into it only if it matches pre-supplied ``expected" values $(x, y)$, i.e, if current value of $v$ are exactly $(x, y)$, then the operation $v.CAS((x,y), (a, b))$ succeeds, and the value of $v$ is changed to be $(a, b)$ and $true$ is returned. Otherwise, the operation fails, and the current value of $v$ remains unchanged and $false$ is returned.

In addition, each process has a unique identifier $i$ which is from an unbounded namespace.  A process can execute local coin flip operations that returns an integer value distributed uniformly at random from an arbitrary finite set of integers. In the following discussion, we use $random(s)$ to return a value distributed uniformly at random from the set $\{0, 1, 2,..., s-1\}$. We analyze our algorithm under the assumption of a strong adaptive adversary. At any point of time, it can see the entire history and know the states of all processes. Base on such history, it decides which process takes the next step.

\section{Problem Statement}

Dynamic task allocation is the problem in which $n$ processes perform a set of tasks cooperatively and the tasks are inserted dynamically by the adversary during the execution.

In theoretical computer science, a concurrent object has a $name$ and a $type$. A $type$ is defined by (1) a set of possible values for objects of that type, (2) a finite set of operations through which the objects of that type could be manipulated, (3) a sequential specification describing, for each operation, the condition under which that operation can be invoked, and the effect produced after the operation was executed.

Our dynamic task allocation type $DTA$ supports two operations, i.e, $DoTask($ $)$ and $InsertTask( \ell )$, where $\ell$ is a task identifier which is unique for each task. The input of the system is a string of $DoTask($ $)$ and $InsertTask( \ell )$ operations to be executed. When a process completes its current operation, it is assigned the next operation in the string.

We formalize the notion of type $DTA$ by specifying the above two operations. For clarity, we define the concepts that a task is inserted and performed. We say a task $\ell$ is inserted if it is associated to a shared memory location $M$. we assume that there exists a atomic operation $PutTask(M,\ell)$, and a process could associate task $\ell$ with memory location $M$ by calling $PutTask(M,\ell)$. It returns $success$ if the the task $l$ has already been associated with the location, and returns $failure$ if the location was already associated with another task. Similarly, We assume there exists an atomic operation $TryTask(M)$, and the task $\ell$ associated with memory location $M$ could be performed atomically by a process by calling $TryTask(M)$. Out of several processes calling $TryTask(M)$, only one receives $success$ and the index $\ell$ of that task, while all the others receive $failure$. This assumption guarantees that only one process could actually perform task $l$, i.e, task $l$ could be performed at most once during the executing.

A task is $done$ if its index has been returned by a process after a $DoTask($ $)$ call. A task is $available$ if it has been inserted but not done yet.

Operation $DoTask($ $)$ performs an available task associated to memory location $M$ by calling $TryTask(M)$.  $DoTask($ $)$ performs a task and returns its index $l$ if and only its $TryTask(M)$ operation returns $success$. If there is no available task, then operation $DoTask($ $)$ returns $\perp$.

The $InsertTask( \ell )$ operation inserts task $\ell$ by calling $InsertTask(M, \ell)$, where $M$ is the memory location task $\ell$ will be associated to. The $InsertTask( \ell )$ operation inserts a task successfully and returns $success$ if and only if its $InsertTask(M, \ell)$ returns $success$.

The type $DTA$ is required to satisfy: $Validity$: If a $DoTask($ $)$ returns $\ell$, then before that, an $InsertTask( \ell )$ was executed and returned success. $Uniqueness$: For each task $\ell$, at most one $DoTask($ $)$ returns $\ell$, i.e, each task is performed at most once.

In addition, the property that every inserted task is eventually done is also a desired progress property of the implementation of type $DTA$.

\chapter{The Adaptive To-Do Tree}

\emph{Some description here.}

\begin{algorithm}
\caption{DoTask()}               %标题
\label{alg1}                         %标记算法，方便在其它地方引用
\begin{algorithmic}[1]
\WHILE {(true)}
   \STATE $v \leftarrow root$\\
   \IF {($v.surplus() \leq 0$)}
   \RETURN $\perp$
   \ENDIF \\
   \vspace{3mm}
   /* Descent */
   \WHILE {$v$ is not a leaf}
       \STATE $(x_L, y_L) \leftarrow v.left.read()$\\
       \STATE $(x_R, y_R) \leftarrow v.right.read()$\\
       \STATE $s_L \leftarrow min(x_L-y_L, 2^{height(v)})$ \\
       \STATE $s_R \leftarrow min(x_R-y_R, 2^{height(v)})$ \\
       \STATE $r \leftarrow random(0,1)$
       \IF {($(s_L+s_R)=0$)}
           \STATE Mark-up($v$)
       \ELSIF {($r<s_L/(s_L+s_R)$)}
           \STATE $v \leftarrow v.left$
       \ELSE
           \STATE $v \leftarrow v.rght$
       \ENDIF
   \ENDWHILE
   \vspace{3mm}\\
   /* $v$ is a leaf */
   \STATE $(x,y) \leftarrow v.read()$
   \STATE $(flag, l)$ $\leftarrow$ $v$.TryTask(task$[y+1]$)
   \STATE $v$.CAS$((x, y), (x, y+1))$    // Update Insertion Count
   \STATE $v \leftarrow v.parent$
   \STATE Mark-up($v$)
   \IF {$flag$ = success}
       \RETURN $\ell$
   \ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Insert(task $\ell$)}               %标题
\label{alg2}                         %标记算法，方便在其它地方引用
\begin{algorithmic}[1]
\WHILE {(true)}
   \STATE $v \leftarrow root$\\
   \vspace{3mm}
   /* Descent */
   \WHILE {($v$ is not a leaf)}
       \STATE $(x_L, y_L) \leftarrow v.left.read()$\\
       \STATE $(x_R, y_R) \leftarrow v.right.read()$\\
       \STATE $s_L \leftarrow min(x_L-y_L, 2^{height(v)})$ \\
       \STATE $s_R \leftarrow min(x_R-y_R, 2^{height(v)})$ \\
       \STATE $r \leftarrow random(0,1)$
       \IF {($(s_L+s_R) = 0$)}
           \STATE Mark-up($v$)
       \ELSIF {($r < s_L/(s_L + s_R)$)}
           \STATE $v \leftarrow v.left$
       \ELSE
           \STATE $v \leftarrow v.rght$
       \ENDIF
   \ENDWHILE
   \vspace{3mm}\\
   /* $v$ is a leaf */
   \STATE $(x,y) \leftarrow v.read()$
   \STATE $flag$ $\leftarrow$ $v$.PutTask(task$[x+1]$, $\ell$)
   \STATE $v$.CAS$((x,y), (x+1,y))$ // Update Insertion Count
   \STATE $v \leftarrow v.parent$
   \STATE Mark-up($v$)
   \IF {($flag$ = success)}
       \RETURN $success$
   \ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Mark-up($v$)}               %标题
\label{alg3}                         %标记算法，方便在其它地方引用
\begin{algorithmic}[1]
\IF{($v$ is not Null)}
    \FOR {($i=0; i<2; i++$)}
    \STATE $(x, y) \leftarrow v.read()$
    \STATE $(x_L, y_L) \leftarrow v.left.read()$
    \STATE $(x_R, y_R) \leftarrow v.right.read()$
    \STATE $v$.CAS$((x,y), (max(x, x_L + x_R), max(y, y_L + y_R))$\\
    \ENDFOR
\ENDIF
\end{algorithmic}
\end{algorithm}
%--------------------CHAPTER 3-----------------------------
\theoremstyle{definition}
\chapter{Analysis}
\section{Correctness}
The standard correctness condition for shared memory algorithms is linearizability, which was introduced by Herlihy and Wing in 1990. The intuition of linearizability is that real-time behavior of method calls must be preserved, i.e, if one method call precedes another, then the earlier call must have taken effect before the later one. By contrast, if two method calls overlap, we are free to order them in any convenient way since the order is ambiguous. Informally, a concurrent object is linearizable if each method call appears to take effect instantaneously at some moment between its invocation and response.

In the next section, we will first build the formal definition and terminology for linearizability, and then prove that our concurrent dynamic task allocation object is linearizable.

\subsection{Definitions and Terminology}
%An implementation is linearizable if for every execution, there is a total order of all completed operations and a subset of the uncompleted operations in the execution that satisfies the sequential specifications of the object and is consistent with the real-time order of these operations. The basic rule behind Linearizability is that every concurrent history is equivalent to some sequential history. If one method call proceeds another, then the other one mush have taken effect before the later one call. By contrast, if two method calls overlap, then we are free to order them in any convenient way.

\begin{definition}{\textbf{History}.}
A history  (or execution) $H$, obtained by processes executing shared memory operations on concurrent objects, is a finite sequence of method invocation and response events.
\end{definition}

A history $H$ may be related to several concurrent objects. When we consider the concurrency behavior of a specific object, we should focus on the subhistory of that object which is defined as follows:

\begin{definition}
$H|obj$ of history $H$ is the subsequence of all invocation and response events in $H$ whose object names are all $obj$.
\end{definition}

If all invocation and response events in a history $H$ have the same object name $obj$, then the $H|obj = H$. Thus, in the following discussion, when we discuss the concurrency behavior of a specific objet $obj$, the history $H$ and $H|obj$ are the same.

We use $inv_H(op)$ to denote the position of the invocation of operation $op$ in history $H$. Similarly we use $rep_H(op)$ to denote the position of the corresponding response of $op$ in $H$. Actually, for each operation $op$, it is not necessary to have a matching response. In this case, we call the operation $op$ is pending and denote it as $rsp_H(op) = \infty$. Otherwise, we call the operation is complete in $H$.

\begin{definition}{\textbf{Complete/Imcomplete History}.}
A history is complete if all its operations are complete. A completion of an incomplete history $H$ is an extension $H'$ of $H$, such that $H'$ contains exactly the same operations as $H$ but the responses are added for the pending operations in $H$.
\end{definition}

Let $H$ be a complete history. With each operation $op$ in $H$ we could associate a time interval $I_H(op) = [inv(op), rsp(op)]$. Similarly, for an uncomplete history, we denote the interval with respect to the pending operation $op$ by $I_H(op) = [inv(op), \infty]$.

\begin{definition}{\textbf{Sequential History}.}
A history is sequential if the first event in the history is an invocation, and each invocation, except possibly the last one, is immediately followed by a corresponding response.
\end{definition}

It is obvious that a sequential history defines a total order over all operations.

\begin{definition}{\textbf{Sequential Specification}.}
The sequential specification $S_{obj}$ for a concurrent object $obj$ is a set of sequential histories on $obj$.
\end{definition}

We say a sequential history $S$ satisfies the sequential specification $S_{obj}$ of object $obj$, it means $S \in S_{obj}$. A sequential history $S$ of object $obj$ is valid (sometimes also called legal) if it satisfies the sequential specification of $obj$.


With the above definitions, we could now define linearization as follows:

\begin{definition}{\textbf{Linearization}.}
A history $H$ linearizes to a sequential history $S$, if and only if $S$ satisfies the following conditions: (1) $S$ and the completion of $H$ have the same operations, (2) sequential history $S$ is valid, and (3) there is a mapping from each time interval $I_H(op)$ to a time point $t_H(op) \in I_H(op)$, such that the sequential history $S$ could be obtained by sorting the operations in $H$ by their $t_H(op)$ values.
\end{definition}

A history is linearizable if and only if there exists a sequential history $S$ that linearizes $H$. In this case, $S$ is called linearization of $H$.

We choose one linearization of $H$ and this linearization defines $t_H(op)$. For each operation $op$ in history $H$, we call the point $t_H(op)$ linearization point of $op$.

\begin{definition}{\textbf{Linearizable Object}.}
A concurrent object is linearizable if every history $H$ of that object is linearizable.
\end{definition}

In the following subsection, we will prove that the concurrent dynamic task allocation object shown in Figure 1 is linearizable.



\subsection{Analysis and Proofs}

By the definitions in Subsection 3.1.1, one way to show a concurrent object $obj$ is linearizable is to prove every history $H$ of $obj$ is linearizable. Thus, we need to identify for each DoTask and InsertTask operation $op$ (i.e, interval $I_H(op)$) in $H$ a linearization point $t_H(op)$, and prove that the sequential history $S$ obtained by sorting these operations according to their $t_H(op)$ satisfies the sequential specification  $S_{obj}$  of $obj$.

We notice that each complete DoTask or InsertTask operation can be associated with a unique task array slot based on the task it removed or inserted. Additionally, the removal and insertion count are both monotonically increasing. Thus, we could associate the node counts with operations which have been propagated to that node.

Now we define ``an operation is counted at a node" recursively to formalize the operation propagation.

A DoTask operation is counted at leaf $v$ when the removal count of $v$ is updated with the index of the task array slot where the performed task is located. Symmetrically, an InsertTask operation is counted at $v$ when the insertion count of $v$ is updated with the index of the task array slot where the inserted task is located.

Now we only define DoTask operation is counted at an inner node $v$ because counting an InsertTask operation is symmetric as well.

Recall that the removal count of $v$ is updated though CAS operation (line 6, method 3). Actually there could be more than one operations updating the count with the same value. We linearize all such CAS operations, which update the removal count of $v$ with the same value $y$. We say for all these operations, only the first one in the linearization order counts the corresponding DoTask operation. In another word, a DoTask operation is counted at an inner node $v$ as soon as the CAS updating operation that counts the DoTask is linearized. Based this definition, no operation will be counted twice at a node.

Please note that, the CAS operation counting the DoTask at node $v$ is not necessary performed by the DoTask operation itself, i.e, suppose process $p$ executes a DoTask operation and has successfully performed task $\ell$ at certain leaf. Then the CAS operation counting this $p.DoTask()$ at node $v$ could be a different process $q$ as long as $q$ updates the removal count first in the linearization order.

Given the above concepts and properties, we could prove the following result:

\begin{lem}
Let $v$ be a tree node, \\
(1) If $(x, y)$ is the return value of $v.read( )$, then there exists a set of $x$ InsertTask operations and a set of $y$ DoTask operation that have been counted at node $v$ by the end of the execution of $v.read( )$.\\
\noindent (2) If there are $x$ DoTask operations that have been counted at $v$ before the execution of $v.read( )$, then the removal count value returned by $v.read( )$ is not less than $x$.
\end{lem}
\begin{proof}
TBD
%[Sketch] This is ensured by the repetition of double compare and swap operation during the execution of mark-up($v$).
\end{proof}



\begin{lem}
Consider a history $H$ and an arbitrary operation $op$ in $H$, let $t_H(op)$ be the point when $op$ is counted at the root, then $t_H(op)$ is between $inv_H(op)$ and $rsp_H(op)$.
\end{lem}
\begin{proof}
Without loss of generality, suppose process $p$ executes DoTask operation $op$ which has performed task $\ell$ successfully at the leaf. When it reaches the $root$ and executes Mark-up($root$), there will be two cases:

Case 1:  It increments the removal count of $root$ successfully via CAS (line 6, method 3) at point $\tau$. If it is the first one in the linearization order of all CAS operation updating of the removal count with the same value, then $t_H(op) = \tau$, therefore $inv_H(op) < t_H(op) < rsp_H(op)$. Otherwise, we could let $t_H(op) = \tau'$, where $\tau'$ is the time when the first CAS operation in the linearization order updated the removal count. Thus $t_H(op) < \tau < rsp_H(op)$, Because only if the task has been performed then the removal count of root could be updated. so $t_H(op) > inv_H(op)$. Therefore, in this case, $inv_H(op) < t_H(op) < rsp_H(op)$ holds.

Case 2: It fails to increment the removal count. The CAS operation of $p$ fails if and only if the value of the counts were updated by another process at a point before $\tau$, suppose it is $\tau'$. Please note that, we say the counts were updated, it means the removal count or the insertion count was updated because the two counts are stored in one memory location. Thus, there are two subcases.

Subcase 2.1: If it is the removal count that was incremented at $\tau'$, it means the DoTask operation has already been counted by another process. Thus, $t_H(op) \leq \tau' < rsp_H(op)$.

Subcase 2.2: If it is not the removal count but the insertion count that was updated at $\tau'$. We notice that the CAS operation (line 6, method 3) will be repeated by $p$, during the second iteration, if the CAS of $p$ succeed at $\tau''$, then we could deduce that $t_H(op) \leq \tau''$, therefore  $t_H(op) < rsp_H(op)$. If it fails again at point $\tau''$ , then there must be another process updated the counts of root again. This time, the counts of the children will be noticed and the DoTask operation will propagate to the root. We could deduce $t_H(op) < \tau''$. Thus,  $t_H(op) < rsp_H(op)$ as well.
\end{proof}

Under the above definitions and properties, we claim the point $t_H(op)$ when $op$ is counted at the root is the linearization point of operation $op$.

\begin{lem}
The dynamic task allocation object in the above figure is linearizable.
\end{lem}
\begin{proof}
Consider an arbitrary history $H$ containing DoTask and InsertTask operations. We should prove for any execution of our algorithm, the total order given by the linearization point is verifies the uniqueness and validity. If $H$ is not complete, then we let all processes that have not finished their operations continue to take steps in an arbitrary order until all operations are completed. Every operation will finally be done is ensured by our computation model and the randomness of our algorithm. This way we obtain a completion $H'$ of $H$ and it suffices to prove $H'$ is linearizable. Thus, to prove this lemma, we should prove the total order obtained by sorting the operations by their $t_H(op)$ values is valid.

\texttt{The uniqueness is obvious. When multiple processes are calling TryTask($\ell$) at the memory location, only one of them will receive success and the index $\ell$ of the task, all the other competitor processes will get failure. Task $\ell$ is performed successfully as long as the value of corresponding memory location is turned to 1. The following process will never repeatedly turn it be to 0 and turn 0 to 1 which is guaranteed by the our semantics of task insertion and removing.}

Now we prove the validity, i.e. each task that is performed successfully must have been inserted before. We should prove the insertion operation of a task is always counted at the root before the removal operation. To prove this result holds for the root, we now prove it by induction from the leaf.

At the leaf, this holds because if and only if the insertion count of newly inserted task $\ell$ has been incremented (line 19, method 2) then the following removal operation could read that (line 20, method 1) to know the available task $\ell$ at the leaf and then try to perform it. In another word, suppose the task $\ell$ is inserted but the insertion count is not incremented (i.e. insertion has not been counted yet), then the following removal operation has no way to know the available task $\ell$, perform it and increment the removal count. Thus, the removal will not be counted.

For an arbitrary inner node $v$, we suppose, by the induction step and lemma 1, the result holds for children $v.left$ and $v.right$. We could notice that any process updates the insertion count and removal count as an atomic operation (line 6, method 3). If some remove operation has been counted at node $v$, then the corresponding insert operation for that task must have been counted at $v$ simultaneously by the double compare-and-swap operation. Apply this to the root, then validity condition holds.
\end{proof}
\section{Performance}
\subsection{DoTask Analysis}
\subsection{InsertTask Analysis}
\section{Competitive Analysis}
%--------------------CHAPTER 1-----------------------------
\chapter{Conclusions}
\section{Contributions}
\section{Future Work}

